{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d00c7a2c"
      },
      "source": [
        "# BiLSTM\n",
        "\n",
        "First we import the necessary libraries."
      ],
      "id": "d00c7a2c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bf15c18",
        "outputId": "4ea50e02-8501-4542-e4c2-3b6be3e787ea"
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import spacy\n",
        "nltk.download('wordnet')\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, SpatialDropout1D, Conv1D, MaxPooling1D, GRU, BatchNormalization\n",
        "from tensorflow.keras.layers import Input, Bidirectional, GlobalAveragePooling1D, concatenate, LeakyReLU, GlobalMaxPooling1D, Flatten\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "0bf15c18",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20bGE1Hr1LEG"
      },
      "source": [
        "We mount the drive."
      ],
      "id": "20bGE1Hr1LEG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAzP-X5M1QGW",
        "outputId": "10a95d9f-5ad9-4a1d-a613-c074efe0e394"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "wAzP-X5M1QGW",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3e6a3b7"
      },
      "source": [
        "We also import the data."
      ],
      "id": "b3e6a3b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20b96f21"
      },
      "source": [
        "# Train data \n",
        "traindata_all = pd.read_csv('/content/drive/MyDrive/TFM/multimodal_train.tsv',sep='\\t')\n",
        "# Validation data \n",
        "validata_all = pd.read_csv('/content/drive/MyDrive/TFM/multimodal_validate.tsv',sep='\\t')\n",
        "# Test data \n",
        "testdata_all = pd.read_csv('/content/drive/MyDrive/TFM/multimodal_test_public.tsv',sep='\\t')"
      ],
      "id": "20b96f21",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23c42c04"
      },
      "source": [
        "We select a subset of the dataframe with no missing values in the 'clean_title' column."
      ],
      "id": "23c42c04"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01cb15cc"
      },
      "source": [
        "# Train data with no missing values\n",
        "train_data = traindata_all[traindata_all['clean_title'].notnull().to_numpy()]\n",
        "# Validation data with no missing values\n",
        "valid_data = validata_all[validata_all['clean_title'].notnull().to_numpy()]\n",
        "# Test data with no missing values\n",
        "test_data = testdata_all[testdata_all['clean_title'].notnull().to_numpy()]"
      ],
      "id": "01cb15cc",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f10f1d0c"
      },
      "source": [
        "And we separate the datasets into the texts and the labels."
      ],
      "id": "f10f1d0c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7de4c17f"
      },
      "source": [
        "## Train data\n",
        "train_news = list(train_data['clean_title'])\n",
        "train_labels = list(train_data['6_way_label'])\n",
        "## Valid data\n",
        "valid_news = list(valid_data['clean_title'])\n",
        "valid_labels = list(valid_data['6_way_label'])\n",
        "## Test data\n",
        "test_news = list(test_data['clean_title'])\n",
        "test_labels = list(test_data['6_way_label'])"
      ],
      "id": "7de4c17f",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afca6ec9"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We define a function to preprocess the data. We remove punctuations and numbers and also multiple spaces."
      ],
      "id": "afca6ec9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60439482"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "id": "60439482",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7f5bc0c"
      },
      "source": [
        "# Remove puntuations and numbers and multiple spaces\n",
        "\n",
        "train_news_clean_1 = []\n",
        "valid_news_clean_1 = []\n",
        "test_news_clean_1 = []\n",
        "# Train\n",
        "for new in train_news:\n",
        "    train_news_clean_1.append(preprocess_text(new))\n",
        "# Validation\n",
        "for new in valid_news:\n",
        "    valid_news_clean_1.append(preprocess_text(new))\n",
        "# Test\n",
        "for new in test_news:\n",
        "    test_news_clean_1.append(preprocess_text(new))"
      ],
      "id": "e7f5bc0c",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdd34e23"
      },
      "source": [
        "Now we create a function to remove stop words and perform lemmatization."
      ],
      "id": "bdd34e23"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67bbce01"
      },
      "source": [
        "# Funtion to remove stop-words and perform lemmatization\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords_lem(text):\n",
        "    text = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    # Lematization\n",
        "    lemmatized_text = []\n",
        "    for word in text:\n",
        "        word1 = lemmatizer.lemmatize(word, pos = \"n\")\n",
        "        word2 = lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "        word3 = lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "        lemmatized_text.append(word3)\n",
        "        \n",
        "    text_done = ' '.join(lemmatized_text)\n",
        "    return text_done"
      ],
      "id": "67bbce01",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9c7fe06"
      },
      "source": [
        "We remove stop words and perform lemmatization."
      ],
      "id": "a9c7fe06"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b23b3ac"
      },
      "source": [
        "# Stop-words removal and lemmatization\n",
        "train_stwrd_lem = []\n",
        "valid_stwrd_lem = []\n",
        "test_stwrd_lem = []\n",
        "\n",
        "# Train\n",
        "for new in train_news_clean_1:\n",
        "    train_stwrd_lem.append(remove_stopwords_lem(new))\n",
        "# Validation\n",
        "for new in valid_news_clean_1:\n",
        "    valid_stwrd_lem.append(remove_stopwords_lem(new))\n",
        "# Test\n",
        "for new in test_news_clean_1:\n",
        "    test_stwrd_lem.append(remove_stopwords_lem(new))"
      ],
      "id": "6b23b3ac",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0977f7d9"
      },
      "source": [
        "We train a tokenizer using all the documents and we used the learned vocabulary in order to transform texts into sequences of ID's."
      ],
      "id": "0977f7d9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "611ac1ad"
      },
      "source": [
        "news_all = train_stwrd_lem + valid_stwrd_lem + test_stwrd_lem\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 128022)\n",
        "tokenizer.fit_on_texts(news_all)\n",
        "\n",
        "# Tokenize news\n",
        "\n",
        "# Train\n",
        "train_tokenized = tokenizer.texts_to_sequences(train_stwrd_lem)\n",
        "# Validation\n",
        "valid_tokenized = tokenizer.texts_to_sequences(valid_stwrd_lem)\n",
        "# Test\n",
        "test_tokenized = tokenizer.texts_to_sequences(test_stwrd_lem)"
      ],
      "id": "611ac1ad",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjWuUm1VQh2H"
      },
      "source": [
        "Obtain the vocabulary length."
      ],
      "id": "gjWuUm1VQh2H"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VRIA8uAQlBw",
        "outputId": "c457bef2-0fba-41b0-bda2-6ac8e66c977d"
      },
      "source": [
        "print(\"Vocabulary length: \", len(tokenizer.word_index))"
      ],
      "id": "2VRIA8uAQlBw",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary length:  117129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef0a704d"
      },
      "source": [
        "Now we pad the sequences of numbers generated by the tokenizer. But firstly we check how many sequences are shorter than a given length. We start by defining a function that counts the length of each sequence."
      ],
      "id": "ef0a704d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a41f395"
      },
      "source": [
        "# Function to count the lenght of each sequence\n",
        "def length_squences(data):\n",
        "    lengths = []\n",
        "    for i in range(len(data)):\n",
        "        lengths.append(len(data[i]))\n",
        "    return lengths"
      ],
      "id": "1a41f395",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8194622"
      },
      "source": [
        "Now we count the lenghts of all the sequences in the training, validation and test set and we check what % of sequences in the train, validation and test sets are smaller than a given length when they are tokenized."
      ],
      "id": "b8194622"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyt56R4boxIC",
        "outputId": "03e7bcca-0741-4a1c-a648-89bdfdbf9510"
      },
      "source": [
        "length = [10, 15, 20, 25]\n",
        "\n",
        "# Train set\n",
        "lengths_train = np.array(length_squences(train_tokenized))\n",
        "perc_length_train = []\n",
        "for lgth in length:\n",
        "   perc_length_train.append( sum(lengths_train < lgth)/len(lengths_train)*100)\n",
        "print(\"Pertentages (train):\", perc_length_train)\n",
        "\n",
        "# Validation set \n",
        "lengths_valid = np.array(length_squences(valid_tokenized))\n",
        "perc_length_valid = []\n",
        "for lgth in length:\n",
        "   perc_length_valid.append( sum(lengths_valid < lgth)/len(lengths_valid)*100)\n",
        "print(\"Pertentages (validation):\", perc_length_valid)\n",
        "\n",
        "# Test set\n",
        "lengths_test = np.array(length_squences(test_tokenized))\n",
        "perc_length_test = []\n",
        "for lgth in length:\n",
        "   perc_length_test.append( sum(lengths_test < lgth)/len(lengths_test)*100)\n",
        "print(\"Pertentages (test):\", perc_length_test)"
      ],
      "id": "Vyt56R4boxIC",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pertentages (train): [91.42251773049645, 98.45230496453901, 99.49361702127659, 99.81382978723404]\n",
            "Pertentages (validation): [91.54898722658487, 98.43449833170436, 99.49614101311045, 99.80957837619225]\n",
            "Pertentages (test): [91.31812741280197, 98.44063453530909, 99.50268885180128, 99.8179335457442]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlKJtEiAox_K"
      },
      "source": [
        "We plot the results."
      ],
      "id": "LlKJtEiAox_K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "a97Qwcfxo346",
        "outputId": "71830bae-7c50-451d-8b3e-89d1111ac5c0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# create dataset\n",
        "bars = [91.42, 91.55, 91.32, 98.45, 98.43, 98.44, 99.49, 99.50, 99.50, 99.81, 99.81, 99.82]\n",
        "labels = ['< 10', '< 15', '< 20', '< 25']\n",
        "\n",
        "x_pos_bars = [1, 2, 3, 6, 7, 8, 11, 12, 13, 16, 17, 18]\n",
        "x_pos_labels = [2, 7, 12, 17]\n",
        "\n",
        "# Colors and mapping to values\n",
        "colors = ['b', 'g', 'y']\n",
        "colors_values = {'Train':'b', 'Validation':'g', 'Test': 'y'}    \n",
        "\n",
        "labels2 = list(colors_values.keys())\n",
        "handles = [plt.Rectangle((0,0),1,1, color=colors_values[label]) for label in labels2]\n",
        "# Make the plot\n",
        "plt.bar(x_pos_bars, bars,color = colors, label = colors_values)\n",
        "\n",
        "# Create names on the x-axis\n",
        "plt.xticks(x_pos_labels, labels)\n",
        "plt.legend(handles, labels2)\n",
        "plt.ylim([0, 130])\n",
        "#plt.xlabel('Metrics')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title('% of texts smaller than a given length')\n",
        "plt.show()"
      ],
      "id": "a97Qwcfxo346",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75be4fac"
      },
      "source": [
        "As we can see almost all news are smaller than **15** in length when tokenized so choosing this lenght to truncate the tokenized news will not eliminate any information from the news in almost any case."
      ],
      "id": "75be4fac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d51500c"
      },
      "source": [
        "# Pad the tokenized news\n",
        "\n",
        "# Train\n",
        "train_tokenized_pad = pad_sequences(train_tokenized, maxlen = 15, truncating = 'post', padding = 'post')\n",
        "# Validation\n",
        "valid_tokenized_pad = pad_sequences(valid_tokenized, maxlen = 15, truncating = 'post', padding = 'post')\n",
        "# Test\n",
        "test_tokenized_pad = pad_sequences(test_tokenized, maxlen = 15, truncating = 'post', padding = 'post')"
      ],
      "id": "9d51500c",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1a6107"
      },
      "source": [
        "In order to use the data with torch we have to transform the arrays into dataloader objects but first they need to be transformed into tensors."
      ],
      "id": "ab1a6107"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "523ddc86"
      },
      "source": [
        "# Transform data arrays into tensors\n",
        "\n",
        "# Train\n",
        "train_tensor = torch.Tensor(train_tokenized_pad).int()\n",
        "# Validation\n",
        "valid_tensor = torch.Tensor(valid_tokenized_pad).int()\n",
        "# Test\n",
        "test_tensor =  torch.Tensor(test_tokenized_pad).int()\n",
        "\n",
        "# Tranform tensors into data loader objects\n",
        "\n",
        "# Train\n",
        "train_set = TensorDataset(train_tensor, torch.Tensor(np.array(train_labels)))\n",
        "trainloader = DataLoader(train_set, batch_size=60)\n",
        "# Validation\n",
        "valid_set = TensorDataset(valid_tensor, torch.Tensor(np.array(valid_labels)))\n",
        "validloader = DataLoader(valid_set, batch_size=60)\n",
        "# Test\n",
        "test_set = TensorDataset(test_tensor, torch.Tensor(np.array(test_labels)))\n",
        "testloader =  DataLoader(test_set, batch_size=60)"
      ],
      "id": "523ddc86",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0948b8e1"
      },
      "source": [
        "### Word embeddings\n",
        "\n",
        "We will try different word embeddings an select the one which performs better. We start by creating two functions: one which will be used to load the word embeddings and the other which will create the embedding matrix that we will feed to the embedding layer of the models."
      ],
      "id": "0948b8e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a936eef2"
      },
      "source": [
        "# Function to load the word embeddings\n",
        "\n",
        "def load_embedd(filename):\n",
        "    words = []\n",
        "    vectors = []\n",
        "    file = open(filename,'r', encoding=\"utf8\")\n",
        "    for line in file.readlines():\n",
        "       row = line.split(' ')\n",
        "       vocab = row[0]\n",
        "       embd = row[1:len(row)]\n",
        "       embd[-1] = embd[-1].rstrip()\n",
        "       embd = list(map(float,embd)) # convert string to float\n",
        "       words.append(vocab)\n",
        "       vectors.append(embd)\n",
        "    file.close()\n",
        "    return words,vectors"
      ],
      "id": "a936eef2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60b73356"
      },
      "source": [
        "Function to create the embedding matrix."
      ],
      "id": "60b73356"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93e41813"
      },
      "source": [
        "# Function to create the embedding matrix\n",
        "\n",
        "def embed_matx(word_index, vocab, embeddings, length_vocab, length_embedding):\n",
        "    embedding_matrix = np.zeros((length_vocab +1, length_embedding))\n",
        "    for word, i in word_index.items():\n",
        "        if word in vocab:\n",
        "            idx = vocab.index(word)\n",
        "            vector =  embeddings[idx]\n",
        "            embedding_matrix[i] = vector\n",
        "        if i == length_vocab:\n",
        "            break\n",
        "    return embedding_matrix"
      ],
      "id": "93e41813",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9e9b9e3"
      },
      "source": [
        "#### Glove (300 d)\n",
        "\n",
        "We use GloVe embeddings of dimension 300."
      ],
      "id": "b9e9b9e3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "497a470e"
      },
      "source": [
        "vocab_gv_300, vectors_gv_300 = load_embedd(filename = \"/content/drive/MyDrive/TFM/glove.6B.300d.txt\")"
      ],
      "id": "497a470e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16630476"
      },
      "source": [
        "Now we create the embbeding matrix"
      ],
      "id": "16630476"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a861b86"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# Embedding matrix\n",
        "embedding_matrix_gv_300 = embed_matx(word_index = word_index, vocab = vocab_gv_300, embeddings = vectors_gv_300, \n",
        "                             length_vocab = 117129, length_embedding = 300)"
      ],
      "id": "5a861b86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54316450"
      },
      "source": [
        "## Models\n",
        "\n",
        "First we define the BiLSTM that we will use."
      ],
      "id": "54316450"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95c12b67"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, nlabels, train_parameters = True, random_embeddings = True): \n",
        "        super().__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings = 117130, embedding_dim = 300)\n",
        "        \n",
        "        if random_embeddings == True:\n",
        "            self.embedding.weight = nn.Parameter(torch.rand(117130, 300), requires_grad = train_parameters)\n",
        "        else:\n",
        "            self.embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix_gv_300), requires_grad = train_parameters)\n",
        "\n",
        "        #  convolutional layer 1.1\n",
        "        self.conv1_1 = nn.Conv2d(in_channels=1, out_channels = 240,kernel_size = (3,140), \n",
        "                               stride=1, padding=0)\n",
        "        \n",
        "        # BiLSTM layer \n",
        "        \n",
        "        self.lstm1 = nn.LSTM(input_size = 300, hidden_size = 70,num_layers = 1, batch_first = True,  bidirectional = True)\n",
        "   \n",
        "        # Linear layers\n",
        "        self.linear1 = nn.Linear(240,64)\n",
        "  \n",
        "        self.linear2 = nn.Linear(64,nlabels)\n",
        "    \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        x = self.embedding(x)\n",
        "        # BiLSTM ayer\n",
        "        x_1, _ = self.lstm1(x.float())\n",
        "        # Convolutional layer2\n",
        "        x_1 = self.conv1_1(x_1.unsqueeze(1).float()).squeeze(3) \n",
        "        x_1 = self.relu(x_1)\n",
        "        # Max pool\n",
        "        x_1 = F.max_pool1d(x_1, x_1.size(2)).squeeze(2)\n",
        "        # Linear layers\n",
        "        x = self.linear1(x_1)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.logsoftmax(x) \n",
        "        return x"
      ],
      "id": "95c12b67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f58cfe00"
      },
      "source": [
        "class BiLSTM_extended(BiLSTM):\n",
        "    \n",
        "    def __init__(self,nlabels, train_parameters, random_embeddings, epochs=100, lr=0.001):\n",
        "        \n",
        "        super().__init__(nlabels, train_parameters, random_embeddings)  \n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        \n",
        "        self.epochs = epochs\n",
        "        \n",
        "        self.criterion = nn.NLLLoss()               \n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "\n",
        "        self.valid_loss_during_training = []\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def trainloop(self,trainloader, validloader):\n",
        "        \n",
        "        # Optimization Loop\n",
        "        \n",
        "        for e in range(int(self.epochs)):\n",
        "\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Random data permutation at each epoch\n",
        "            \n",
        "            running_loss = 0.\n",
        "            \n",
        "            i = 0\n",
        "            \n",
        "            length = 0\n",
        "            \n",
        "            accuracies = []\n",
        "            \n",
        "            for news, labels in trainloader:  \n",
        "\n",
        "                 # Move input and label tensors to the default device\n",
        "                news, labels = news.to(self.device), labels.to(self.device)           \n",
        "        \n",
        "                self.optim.zero_grad()  # Reset gradients\n",
        "            \n",
        "                out = self.forward(news.int())\n",
        "\n",
        "                loss = self.criterion(out,labels.long())\n",
        "                \n",
        "                loss.backward()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                self.optim.step()\n",
        "                \n",
        "                top_p, top_class = out.topk(1, dim=1)\n",
        "                \n",
        "                equals = (top_class == labels.view(news.shape[0], 1))\n",
        "                \n",
        "                length += news.shape[0]\n",
        "                \n",
        "                accuracies.append(sum(equals)) \n",
        "                \n",
        "                accuracy = sum(accuracies)/length\n",
        "                \n",
        "                i += 1\n",
        "                \n",
        "                if i%1000 == 0:\n",
        "                    print(\" Train accuracy: \", accuracy)\n",
        "       \n",
        "                \n",
        "            self.loss_during_training.append(running_loss/len(trainloader))\n",
        "\n",
        "            # Validation Loss\n",
        "            \n",
        "            with torch.no_grad():            \n",
        "                \n",
        "                running_loss = 0.\n",
        "                \n",
        "                i = 0\n",
        "                \n",
        "                length = 0\n",
        "                \n",
        "                accuracies = []\n",
        "                \n",
        "                for news,labels in validloader:\n",
        "\n",
        "                    # Move input and label tensors to the default device\n",
        "                    news, labels = news.to(self.device), labels.to(self.device)\n",
        "                    \n",
        "                    out = self.forward(news.int())\n",
        "\n",
        "                    loss = self.criterion(out,labels.long())\n",
        "\n",
        "                    running_loss += loss.item()   \n",
        "                    \n",
        "                    top_p, top_class = out.topk(1, dim=1)\n",
        "                \n",
        "                    equals = (top_class == labels.view(news.shape[0], 1))\n",
        "                \n",
        "                    length += news.shape[0]\n",
        "                \n",
        "                    accuracies.append(sum(equals)) \n",
        "                \n",
        "                    accuracy = sum(accuracies)/length\n",
        "                    \n",
        "\n",
        "                print(\" Validation accuracy: \", accuracy)                    \n",
        "                      \n",
        "                self.valid_loss_during_training.append(running_loss/len(validloader))\n",
        "\n",
        "            if(e % 1 == 0): # Every 10 epochs\n",
        "\n",
        "                print(\"Training loss after %d epochs: %f\" \n",
        "                      %(e,self.loss_during_training[-1]), \"Validation loss after %d epochs: %f\" %(e,self.valid_loss_during_training[-1]),\n",
        "                      \"Time per epoch: %f seconds\"%(time.time() - start_time))"
      ],
      "id": "f58cfe00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70c66cef"
      },
      "source": [
        "We train the CNN in **4** different scenarios:\n",
        "\n",
        "  - **Random embeddings** + **Training embeddings**\n",
        "  - **GloVe embeddings** + **Training embeddings**\n",
        "  - **Random embeddings** + **Not training embeddings**\n",
        "  - **GloVe embeddings** + **Not training embeddings**\n",
        "  \n",
        "We train the model for several epochs. This is done in order to select the optimal number of epochs, which occurs when the validation loss stops decreasing and starts increasing (Early Stopping)."
      ],
      "id": "70c66cef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f30d25f0"
      },
      "source": [
        "### Random embeddings  + Training embeddings"
      ],
      "id": "f30d25f0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91b02957",
        "outputId": "5e169c33-b488-4f69-9d0d-e1496e040387"
      },
      "source": [
        "# Initialize model\n",
        "BiLSTM_train_random = BiLSTM_extended(nlabels = 6, epochs=5, lr=0.003, train_parameters = True, random_embeddings = True)\n",
        "# Train model\n",
        "BiLSTM_train_random.trainloop(trainloader, validloader)"
      ],
      "id": "91b02957",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5999])\n",
            " Train accuracy:  tensor([0.6277])\n",
            " Train accuracy:  tensor([0.6447])\n",
            " Train accuracy:  tensor([0.6559])\n",
            " Train accuracy:  tensor([0.6636])\n",
            " Train accuracy:  tensor([0.6699])\n",
            " Train accuracy:  tensor([0.6748])\n",
            " Train accuracy:  tensor([0.6789])\n",
            " Train accuracy:  tensor([0.6824])\n",
            " Validation accuracy:  tensor([0.7135])\n",
            "Training loss after 0 epochs: 0.888133 Validation loss after 0 epochs: 0.810962 Time per epoch: 5854.538054 seconds\n",
            " Train accuracy:  tensor([0.7253])\n",
            " Train accuracy:  tensor([0.7286])\n",
            " Train accuracy:  tensor([0.7309])\n",
            " Train accuracy:  tensor([0.7326])\n",
            " Train accuracy:  tensor([0.7343])\n",
            " Train accuracy:  tensor([0.7358])\n",
            " Train accuracy:  tensor([0.7368])\n",
            " Train accuracy:  tensor([0.7376])\n",
            " Train accuracy:  tensor([0.7381])\n",
            " Validation accuracy:  tensor([0.7124])\n",
            "Training loss after 1 epochs: 0.742461 Validation loss after 1 epochs: 0.824408 Time per epoch: 7212.685507 seconds\n",
            " Train accuracy:  tensor([0.7549])\n",
            " Train accuracy:  tensor([0.7572])\n",
            " Train accuracy:  tensor([0.7581])\n",
            " Train accuracy:  tensor([0.7592])\n",
            " Train accuracy:  tensor([0.7599])\n",
            " Train accuracy:  tensor([0.7602])\n",
            " Train accuracy:  tensor([0.7608])\n",
            " Train accuracy:  tensor([0.7611])\n",
            " Train accuracy:  tensor([0.7612])\n",
            " Validation accuracy:  tensor([0.7116])\n",
            "Training loss after 2 epochs: 0.686253 Validation loss after 2 epochs: 0.829951 Time per epoch: 7785.464334 seconds\n",
            " Train accuracy:  tensor([0.7685])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "696ef396"
      },
      "source": [
        "### GloVe embeddings  + Training embeddings"
      ],
      "id": "696ef396"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "25e10966",
        "outputId": "995e61e9-bd96-4b84-c4dc-e663ea920c27"
      },
      "source": [
        "# Initialize model\n",
        "BiLSTM_train_not_random = BiLSTM_extended(nlabels = 6, epochs=4, lr=0.003, train_parameters = True, random_embeddings = False)\n",
        "# Train model\n",
        "BiLSTM_train_not_random.trainloop(trainloader, validloader)"
      ],
      "id": "25e10966",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.6648])\n",
            " Train accuracy:  tensor([0.6859])\n",
            " Train accuracy:  tensor([0.6958])\n",
            " Train accuracy:  tensor([0.7027])\n",
            " Train accuracy:  tensor([0.7080])\n",
            " Train accuracy:  tensor([0.7122])\n",
            " Train accuracy:  tensor([0.7157])\n",
            " Train accuracy:  tensor([0.7184])\n",
            " Train accuracy:  tensor([0.7209])\n",
            " Validation accuracy:  tensor([0.7421])\n",
            "Training loss after 0 epochs: 0.785417 Validation loss after 0 epochs: 0.731576 Time per epoch: 5656.640536 seconds\n",
            " Train accuracy:  tensor([0.7695])\n",
            " Train accuracy:  tensor([0.7776])\n",
            " Train accuracy:  tensor([0.7805])\n",
            " Train accuracy:  tensor([0.7824])\n",
            " Train accuracy:  tensor([0.7839])\n",
            " Train accuracy:  tensor([0.7849])\n",
            " Train accuracy:  tensor([0.7855])\n",
            " Train accuracy:  tensor([0.7860])\n",
            " Train accuracy:  tensor([0.7867])\n",
            " Validation accuracy:  tensor([0.7400])\n",
            "Training loss after 1 epochs: 0.610808 Validation loss after 1 epochs: 0.762398 Time per epoch: 5298.509585 seconds\n",
            " Train accuracy:  tensor([0.8056])\n",
            " Train accuracy:  tensor([0.8102])\n",
            " Train accuracy:  tensor([0.8114])\n",
            " Train accuracy:  tensor([0.8130])\n",
            " Train accuracy:  tensor([0.8134])\n",
            " Train accuracy:  tensor([0.8138])\n",
            " Train accuracy:  tensor([0.8137])\n",
            " Train accuracy:  tensor([0.8137])\n",
            " Train accuracy:  tensor([0.8140])\n",
            " Validation accuracy:  tensor([0.7336])\n",
            "Training loss after 2 epochs: 0.534342 Validation loss after 2 epochs: 0.804783 Time per epoch: 5335.920779 seconds\n",
            " Train accuracy:  tensor([0.8242])\n",
            " Train accuracy:  tensor([0.8267])\n",
            " Train accuracy:  tensor([0.8271])\n",
            " Train accuracy:  tensor([0.8274])\n",
            " Train accuracy:  tensor([0.8274])\n",
            " Train accuracy:  tensor([0.8279])\n",
            " Train accuracy:  tensor([0.8276])\n",
            " Train accuracy:  tensor([0.8276])\n",
            " Train accuracy:  tensor([0.8279])\n",
            " Validation accuracy:  tensor([0.7316])\n",
            "Training loss after 3 epochs: 0.491852 Validation loss after 3 epochs: 0.835323 Time per epoch: 5312.562149 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71489777"
      },
      "source": [
        "### Random embeddings  + Not training embeddings"
      ],
      "id": "71489777"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a8a58ba",
        "outputId": "78629e16-37d8-4322-9b25-e13b360d9798"
      },
      "source": [
        "# Initialize model\n",
        "BiLSTM_not_train_random = BiLSTM_extended(nlabels = 6, epochs=9, lr=0.003, train_parameters = False, random_embeddings = True)\n",
        "# Train model\n",
        "BiLSTM_not_train_random.trainloop(trainloader, validloader)"
      ],
      "id": "0a8a58ba",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5545])\n",
            " Train accuracy:  tensor([0.5616])\n",
            " Train accuracy:  tensor([0.5648])\n",
            " Train accuracy:  tensor([0.5681])\n",
            " Train accuracy:  tensor([0.5727])\n",
            " Train accuracy:  tensor([0.5762])\n",
            " Train accuracy:  tensor([0.5804])\n",
            " Train accuracy:  tensor([0.5850])\n",
            " Train accuracy:  tensor([0.5893])\n",
            " Validation accuracy:  tensor([0.6140])\n",
            "Training loss after 0 epochs: 1.115259 Validation loss after 0 epochs: 1.082880 Time per epoch: 631.668183 seconds\n",
            " Train accuracy:  tensor([0.6276])\n",
            " Train accuracy:  tensor([0.6283])\n",
            " Train accuracy:  tensor([0.6289])\n",
            " Train accuracy:  tensor([0.6301])\n",
            " Train accuracy:  tensor([0.6307])\n",
            " Train accuracy:  tensor([0.6316])\n",
            " Train accuracy:  tensor([0.6325])\n",
            " Train accuracy:  tensor([0.6331])\n",
            " Train accuracy:  tensor([0.6339])\n",
            " Validation accuracy:  tensor([0.6319])\n",
            "Training loss after 1 epochs: 1.018149 Validation loss after 1 epochs: 1.029225 Time per epoch: 686.650449 seconds\n",
            " Train accuracy:  tensor([0.6419])\n",
            " Train accuracy:  tensor([0.6431])\n",
            " Train accuracy:  tensor([0.6432])\n",
            " Train accuracy:  tensor([0.6433])\n",
            " Train accuracy:  tensor([0.6438])\n",
            " Train accuracy:  tensor([0.6445])\n",
            " Train accuracy:  tensor([0.6449])\n",
            " Train accuracy:  tensor([0.6452])\n",
            " Train accuracy:  tensor([0.6458])\n",
            " Validation accuracy:  tensor([0.6470])\n",
            "Training loss after 2 epochs: 0.986609 Validation loss after 2 epochs: 0.988427 Time per epoch: 769.351703 seconds\n",
            " Train accuracy:  tensor([0.6512])\n",
            " Train accuracy:  tensor([0.6518])\n",
            " Train accuracy:  tensor([0.6517])\n",
            " Train accuracy:  tensor([0.6516])\n",
            " Train accuracy:  tensor([0.6519])\n",
            " Train accuracy:  tensor([0.6524])\n",
            " Train accuracy:  tensor([0.6527])\n",
            " Train accuracy:  tensor([0.6530])\n",
            " Train accuracy:  tensor([0.6534])\n",
            " Validation accuracy:  tensor([0.6493])\n",
            "Training loss after 3 epochs: 0.967034 Validation loss after 3 epochs: 0.984387 Time per epoch: 841.024853 seconds\n",
            " Train accuracy:  tensor([0.6589])\n",
            " Train accuracy:  tensor([0.6596])\n",
            " Train accuracy:  tensor([0.6586])\n",
            " Train accuracy:  tensor([0.6583])\n",
            " Train accuracy:  tensor([0.6586])\n",
            " Train accuracy:  tensor([0.6591])\n",
            " Train accuracy:  tensor([0.6595])\n",
            " Train accuracy:  tensor([0.6597])\n",
            " Train accuracy:  tensor([0.6601])\n",
            " Validation accuracy:  tensor([0.6588])\n",
            "Training loss after 4 epochs: 0.950910 Validation loss after 4 epochs: 0.958233 Time per epoch: 810.433971 seconds\n",
            " Train accuracy:  tensor([0.6633])\n",
            " Train accuracy:  tensor([0.6636])\n",
            " Train accuracy:  tensor([0.6626])\n",
            " Train accuracy:  tensor([0.6626])\n",
            " Train accuracy:  tensor([0.6628])\n",
            " Train accuracy:  tensor([0.6632])\n",
            " Train accuracy:  tensor([0.6636])\n",
            " Train accuracy:  tensor([0.6636])\n",
            " Train accuracy:  tensor([0.6640])\n",
            " Validation accuracy:  tensor([0.6597])\n",
            "Training loss after 5 epochs: 0.940176 Validation loss after 5 epochs: 0.957180 Time per epoch: 784.117330 seconds\n",
            " Train accuracy:  tensor([0.6679])\n",
            " Train accuracy:  tensor([0.6684])\n",
            " Train accuracy:  tensor([0.6670])\n",
            " Train accuracy:  tensor([0.6667])\n",
            " Train accuracy:  tensor([0.6667])\n",
            " Train accuracy:  tensor([0.6671])\n",
            " Train accuracy:  tensor([0.6674])\n",
            " Train accuracy:  tensor([0.6674])\n",
            " Train accuracy:  tensor([0.6677])\n",
            " Validation accuracy:  tensor([0.6636])\n",
            "Training loss after 6 epochs: 0.930344 Validation loss after 6 epochs: 0.945400 Time per epoch: 802.475426 seconds\n",
            " Train accuracy:  tensor([0.6698])\n",
            " Train accuracy:  tensor([0.6701])\n",
            " Train accuracy:  tensor([0.6691])\n",
            " Train accuracy:  tensor([0.6688])\n",
            " Train accuracy:  tensor([0.6689])\n",
            " Train accuracy:  tensor([0.6693])\n",
            " Train accuracy:  tensor([0.6696])\n",
            " Train accuracy:  tensor([0.6694])\n",
            " Train accuracy:  tensor([0.6697])\n",
            " Validation accuracy:  tensor([0.6647])\n",
            "Training loss after 7 epochs: 0.924417 Validation loss after 7 epochs: 0.939462 Time per epoch: 829.733619 seconds\n",
            " Train accuracy:  tensor([0.6717])\n",
            " Train accuracy:  tensor([0.6711])\n",
            " Train accuracy:  tensor([0.6705])\n",
            " Train accuracy:  tensor([0.6706])\n",
            " Train accuracy:  tensor([0.6709])\n",
            " Train accuracy:  tensor([0.6715])\n",
            " Train accuracy:  tensor([0.6715])\n",
            " Train accuracy:  tensor([0.6714])\n",
            " Train accuracy:  tensor([0.6718])\n",
            " Validation accuracy:  tensor([0.6651])\n",
            "Training loss after 8 epochs: 0.917482 Validation loss after 8 epochs: 0.936449 Time per epoch: 836.267457 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ac58fd"
      },
      "source": [
        "### GloVe embeddings  + Not training embeddings"
      ],
      "id": "81ac58fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "461ea9db",
        "outputId": "c12b52f5-77d6-4d81-9deb-15bc56ad08e8"
      },
      "source": [
        "# Initialize model\n",
        "BiLSTM_not_train_not_random = BiLSTM_extended(nlabels = 6, epochs=9, lr=0.003, train_parameters = False, random_embeddings = False)\n",
        "# Train model\n",
        "BiLSTM_not_train_not_random.trainloop(trainloader, validloader)"
      ],
      "id": "461ea9db",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.6477])\n",
            " Train accuracy:  tensor([0.6681])\n",
            " Train accuracy:  tensor([0.6778])\n",
            " Train accuracy:  tensor([0.6845])\n",
            " Train accuracy:  tensor([0.6895])\n",
            " Train accuracy:  tensor([0.6939])\n",
            " Train accuracy:  tensor([0.6970])\n",
            " Train accuracy:  tensor([0.6998])\n",
            " Train accuracy:  tensor([0.7026])\n",
            " Validation accuracy:  tensor([0.7208])\n",
            "Training loss after 0 epochs: 0.834196 Validation loss after 0 epochs: 0.785574 Time per epoch: 647.588372 seconds\n",
            " Train accuracy:  tensor([0.7267])\n",
            " Train accuracy:  tensor([0.7287])\n",
            " Train accuracy:  tensor([0.7290])\n",
            " Train accuracy:  tensor([0.7302])\n",
            " Train accuracy:  tensor([0.7311])\n",
            " Train accuracy:  tensor([0.7323])\n",
            " Train accuracy:  tensor([0.7328])\n",
            " Train accuracy:  tensor([0.7336])\n",
            " Train accuracy:  tensor([0.7346])\n",
            " Validation accuracy:  tensor([0.7241])\n",
            "Training loss after 1 epochs: 0.747840 Validation loss after 1 epochs: 0.777635 Time per epoch: 642.568994 seconds\n",
            " Train accuracy:  tensor([0.7410])\n",
            " Train accuracy:  tensor([0.7435])\n",
            " Train accuracy:  tensor([0.7441])\n",
            " Train accuracy:  tensor([0.7448])\n",
            " Train accuracy:  tensor([0.7457])\n",
            " Train accuracy:  tensor([0.7468])\n",
            " Train accuracy:  tensor([0.7471])\n",
            " Train accuracy:  tensor([0.7474])\n",
            " Train accuracy:  tensor([0.7483])\n",
            " Validation accuracy:  tensor([0.7278])\n",
            "Training loss after 2 epochs: 0.710030 Validation loss after 2 epochs: 0.775867 Time per epoch: 641.202327 seconds\n",
            " Train accuracy:  tensor([0.7513])\n",
            " Train accuracy:  tensor([0.7536])\n",
            " Train accuracy:  tensor([0.7537])\n",
            " Train accuracy:  tensor([0.7544])\n",
            " Train accuracy:  tensor([0.7554])\n",
            " Train accuracy:  tensor([0.7562])\n",
            " Train accuracy:  tensor([0.7565])\n",
            " Train accuracy:  tensor([0.7569])\n",
            " Train accuracy:  tensor([0.7577])\n",
            " Validation accuracy:  tensor([0.7265])\n",
            "Training loss after 3 epochs: 0.683956 Validation loss after 3 epochs: 0.784008 Time per epoch: 643.656391 seconds\n",
            " Train accuracy:  tensor([0.7596])\n",
            " Train accuracy:  tensor([0.7626])\n",
            " Train accuracy:  tensor([0.7628])\n",
            " Train accuracy:  tensor([0.7635])\n",
            " Train accuracy:  tensor([0.7640])\n",
            " Train accuracy:  tensor([0.7644])\n",
            " Train accuracy:  tensor([0.7645])\n",
            " Train accuracy:  tensor([0.7649])\n",
            " Train accuracy:  tensor([0.7655])\n",
            " Validation accuracy:  tensor([0.7251])\n",
            "Training loss after 4 epochs: 0.663083 Validation loss after 4 epochs: 0.790461 Time per epoch: 642.663224 seconds\n",
            " Train accuracy:  tensor([0.7659])\n",
            " Train accuracy:  tensor([0.7685])\n",
            " Train accuracy:  tensor([0.7685])\n",
            " Train accuracy:  tensor([0.7686])\n",
            " Train accuracy:  tensor([0.7690])\n",
            " Train accuracy:  tensor([0.7693])\n",
            " Train accuracy:  tensor([0.7697])\n",
            " Train accuracy:  tensor([0.7700])\n",
            " Train accuracy:  tensor([0.7704])\n",
            " Validation accuracy:  tensor([0.7254])\n",
            "Training loss after 5 epochs: 0.647060 Validation loss after 5 epochs: 0.794715 Time per epoch: 644.041868 seconds\n",
            " Train accuracy:  tensor([0.7715])\n",
            " Train accuracy:  tensor([0.7738])\n",
            " Train accuracy:  tensor([0.7744])\n",
            " Train accuracy:  tensor([0.7745])\n",
            " Train accuracy:  tensor([0.7747])\n",
            " Train accuracy:  tensor([0.7751])\n",
            " Train accuracy:  tensor([0.7752])\n",
            " Train accuracy:  tensor([0.7752])\n",
            " Train accuracy:  tensor([0.7757])\n",
            " Validation accuracy:  tensor([0.7257])\n",
            "Training loss after 6 epochs: 0.631848 Validation loss after 6 epochs: 0.802731 Time per epoch: 642.718422 seconds\n",
            " Train accuracy:  tensor([0.7756])\n",
            " Train accuracy:  tensor([0.7790])\n",
            " Train accuracy:  tensor([0.7789])\n",
            " Train accuracy:  tensor([0.7785])\n",
            " Train accuracy:  tensor([0.7788])\n",
            " Train accuracy:  tensor([0.7789])\n",
            " Train accuracy:  tensor([0.7792])\n",
            " Train accuracy:  tensor([0.7795])\n",
            " Train accuracy:  tensor([0.7800])\n",
            " Validation accuracy:  tensor([0.7240])\n",
            "Training loss after 7 epochs: 0.619435 Validation loss after 7 epochs: 0.825244 Time per epoch: 642.629128 seconds\n",
            " Train accuracy:  tensor([0.7796])\n",
            " Train accuracy:  tensor([0.7839])\n",
            " Train accuracy:  tensor([0.7832])\n",
            " Train accuracy:  tensor([0.7829])\n",
            " Train accuracy:  tensor([0.7835])\n",
            " Train accuracy:  tensor([0.7836])\n",
            " Train accuracy:  tensor([0.7835])\n",
            " Train accuracy:  tensor([0.7835])\n",
            " Train accuracy:  tensor([0.7839])\n",
            " Validation accuracy:  tensor([0.7233])\n",
            "Training loss after 8 epochs: 0.608613 Validation loss after 8 epochs: 0.821688 Time per epoch: 642.922532 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c04a1eee"
      },
      "source": [
        "## Testing performance\n",
        "\n",
        "We join the **train** and **validation** datasets and we train the model with this dataset using the optimal number of epochs. Then we evaluate the model with the **test** set. First we need to modify a bit the class used previously."
      ],
      "id": "c04a1eee"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b748f5f"
      },
      "source": [
        "class BiLSTM_extended(BiLSTM):\n",
        "    \n",
        "    def __init__(self,nlabels, train_parameters, random_embeddings, epochs=100,lr=0.001):\n",
        "        \n",
        "        super().__init__(nlabels, train_parameters, random_embeddings)  \n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        \n",
        "        self.epochs = epochs\n",
        "        \n",
        "        self.criterion = nn.NLLLoss()               \n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "\n",
        "        self.valid_loss_during_training = []\n",
        "        \n",
        "    def trainloop(self,trainloader):\n",
        "        \n",
        "        \n",
        "        # Optimization Loop\n",
        "        \n",
        "        for e in range(int(self.epochs)):\n",
        "\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Random data permutation at each epoch\n",
        "            \n",
        "            running_loss = 0.\n",
        "            \n",
        "            i = 0\n",
        "            \n",
        "            length = 0\n",
        "            \n",
        "            accuracies = []\n",
        "            \n",
        "            for news, labels in trainloader:             \n",
        "        \n",
        "                self.optim.zero_grad()  # Reset gradients\n",
        "            \n",
        "                out = self.forward(news.int())\n",
        "\n",
        "                loss = self.criterion(out,labels.long())\n",
        "                \n",
        "                loss.backward()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                self.optim.step()\n",
        "                \n",
        "                top_p, top_class = out.topk(1, dim=1)\n",
        "                \n",
        "                equals = (top_class == labels.view(news.shape[0], 1))\n",
        "                \n",
        "                length += news.shape[0]\n",
        "                \n",
        "                accuracies.append(sum(equals)) \n",
        "                \n",
        "                accuracy = sum(accuracies)/length\n",
        "                \n",
        "                i += 1\n",
        "                \n",
        "                if i%1000 == 0:\n",
        "                    print(\" Train accuracy: \", accuracy)\n",
        "                \n",
        "            self.loss_during_training.append(running_loss/len(trainloader))\n",
        "            end_time = time.time()\n",
        "            print(\"Time elapsed during epoch:\", end_time - start_time)\n",
        "\n",
        "\n",
        "                \n",
        "    def eval_performance(self,dataloader):\n",
        "        predictions = np.empty((1,1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for news,labels in dataloader:\n",
        "                \n",
        "                logprobs = self.forward(news)  \n",
        "                top_p, top_class = logprobs.topk(1, dim=1)\n",
        "                \n",
        "                top_class_array = np.array(top_class)\n",
        "                predictions = np.concatenate((predictions, top_class_array), axis = 0)\n",
        "                \n",
        "        return predictions[1:]"
      ],
      "id": "4b748f5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65b0f56"
      },
      "source": [
        "Now we join the train and validation sets."
      ],
      "id": "c65b0f56"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7b6de3"
      },
      "source": [
        "# Join train and validation sequences\n",
        "train_valid_tokenized_pad = np.concatenate((train_tokenized_pad, valid_tokenized_pad), axis = 0)\n",
        "# Join train and validation labels\n",
        "train_valid_labels = np.concatenate((np.array(train_labels), np.array(valid_labels)), axis = 0)\n",
        "\n",
        "# Create tensor objects\n",
        "\n",
        "# Train + validation\n",
        "train_valid_tensor = torch.Tensor(train_valid_tokenized_pad).int()\n",
        "# Test\n",
        "test_tensor =  torch.Tensor(test_tokenized_pad).int()\n",
        "\n",
        "# Tranform tensors into data loader objects\n",
        "\n",
        "# Train + validation\n",
        "train_valid_set = TensorDataset(train_valid_tensor, torch.Tensor(np.array(train_valid_labels)))\n",
        "train_valid_loader = DataLoader(train_valid_set, batch_size=60)\n",
        "# Test\n",
        "test_set = TensorDataset(test_tensor, torch.Tensor(np.array(test_labels)))\n",
        "testloader =  DataLoader(test_set, batch_size=60)"
      ],
      "id": "7a7b6de3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7465906b"
      },
      "source": [
        "### Random embeddings  + Training embeddings"
      ],
      "id": "7465906b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63376b7f",
        "outputId": "15848952-0759-4933-84f3-149dc7254610"
      },
      "source": [
        "# Initialize model\n",
        "BiLSTM_test_train_random = BiLSTM_extended(nlabels = 6, epochs=1, lr=0.003, train_parameters = True, random_embeddings = True)\n",
        "# Train model\n",
        "BiLSTM_test_train_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions1 = BiLSTM_test_train_random.eval_performance(testloader)"
      ],
      "id": "63376b7f",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5969])\n",
            " Train accuracy:  tensor([0.6261])\n",
            " Train accuracy:  tensor([0.6435])\n",
            " Train accuracy:  tensor([0.6549])\n",
            " Train accuracy:  tensor([0.6631])\n",
            " Train accuracy:  tensor([0.6699])\n",
            " Train accuracy:  tensor([0.6749])\n",
            " Train accuracy:  tensor([0.6788])\n",
            " Train accuracy:  tensor([0.6825])\n",
            " Train accuracy:  tensor([0.6855])\n",
            "Time elapsed during epoch: 6599.531795501709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7289725",
        "outputId": "88eaf528-61e6-4798-a9b9-7ed2e864815c"
      },
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions1))"
      ],
      "id": "f7289725",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.88      0.78     23507\n",
            "           1       0.58      0.25      0.35      3514\n",
            "           2       0.73      0.44      0.55     11297\n",
            "           3       0.86      0.08      0.14      1224\n",
            "           4       0.73      0.85      0.79     17472\n",
            "           5       0.80      0.45      0.58      2305\n",
            "\n",
            "    accuracy                           0.72     59319\n",
            "   macro avg       0.74      0.49      0.53     59319\n",
            "weighted avg       0.72      0.72      0.69     59319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCNrYtJUJMWL",
        "outputId": "c042d5fe-c8f1-48ed-c865-4bf04c64752e"
      },
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions1, labels = [1,2,3,4,5]))"
      ],
      "id": "hCNrYtJUJMWL",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.58      0.25      0.35      3514\n",
            "           2       0.73      0.44      0.55     11297\n",
            "           3       0.86      0.08      0.14      1224\n",
            "           4       0.73      0.85      0.79     17472\n",
            "           5       0.80      0.45      0.58      2305\n",
            "\n",
            "   micro avg       0.73      0.61      0.66     35812\n",
            "   macro avg       0.74      0.41      0.48     35812\n",
            "weighted avg       0.73      0.61      0.63     35812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL3htDIOJQx6",
        "outputId": "fa54d9e8-bbe4-4d62-fa35-60ca1497af48"
      },
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions1))"
      ],
      "id": "JL3htDIOJQx6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[20754   236   812     2  1619    84]\n",
            " [ 1456   864   162     2   994    36]\n",
            " [ 3778   116  4955     8  2352    88]\n",
            " [  791    54    73    93   196    17]\n",
            " [ 1867   134   583     3 14853    32]\n",
            " [  799    74   174     0   212  1046]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53165b74"
      },
      "source": [
        "### GloVe embeddings  + Training embeddings"
      ],
      "id": "53165b74"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac30ff32",
        "outputId": "76beca27-f107-4755-cb3a-66652ef59c97"
      },
      "source": [
        "# Initialize model\n",
        "BiLSTM_test_train_not_random = BiLSTM_extended(nlabels = 6, epochs=1, lr=0.003, train_parameters = True, random_embeddings = False)\n",
        "# Train model\n",
        "BiLSTM_test_train_not_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_2 = BiLSTM_test_train_not_random.eval_performance(testloader)"
      ],
      "id": "ac30ff32",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.6648])\n",
            " Train accuracy:  tensor([0.6852])\n",
            " Train accuracy:  tensor([0.6958])\n",
            " Train accuracy:  tensor([0.7021])\n",
            " Train accuracy:  tensor([0.7072])\n",
            " Train accuracy:  tensor([0.7120])\n",
            " Train accuracy:  tensor([0.7154])\n",
            " Train accuracy:  tensor([0.7183])\n",
            " Train accuracy:  tensor([0.7210])\n",
            " Train accuracy:  tensor([0.7231])\n",
            "Time elapsed during epoch: 7875.144319057465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d8da17b",
        "outputId": "caa7aba6-a384-44fc-b77b-44f48a1a18f2"
      },
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_2))"
      ],
      "id": "4d8da17b",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.86      0.80     23507\n",
            "           1       0.63      0.41      0.50      3514\n",
            "           2       0.72      0.55      0.63     11297\n",
            "           3       0.57      0.18      0.28      1224\n",
            "           4       0.77      0.84      0.80     17472\n",
            "           5       0.78      0.57      0.66      2305\n",
            "\n",
            "    accuracy                           0.75     59319\n",
            "   macro avg       0.70      0.57      0.61     59319\n",
            "weighted avg       0.74      0.75      0.73     59319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsndyOHNKHKI",
        "outputId": "90b7dcf9-2a7a-4ba5-c03e-a3bb08aee27e"
      },
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_2, labels = [1,2,3,4,5]))"
      ],
      "id": "lsndyOHNKHKI",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.41      0.50      3514\n",
            "           2       0.72      0.55      0.63     11297\n",
            "           3       0.57      0.18      0.28      1224\n",
            "           4       0.77      0.84      0.80     17472\n",
            "           5       0.78      0.57      0.66      2305\n",
            "\n",
            "   micro avg       0.74      0.67      0.70     35812\n",
            "   macro avg       0.69      0.51      0.57     35812\n",
            "weighted avg       0.73      0.67      0.69     35812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj0Qb7YyKHHB",
        "outputId": "9061ed98-6e22-4897-d567-7758514ef7cf"
      },
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_2))"
      ],
      "id": "rj0Qb7YyKHHB",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[20321   317  1184    71  1477   137]\n",
            " [ 1130  1444   156    37   696    51]\n",
            " [ 2814   161  6267    27  1931    97]\n",
            " [  661    40    96   224   171    32]\n",
            " [ 1552   287   899    24 14645    65]\n",
            " [  599    56   155    12   159  1324]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaeb7009"
      },
      "source": [
        "### Random embeddings  +  Not training embeddings"
      ],
      "id": "aaeb7009"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ce1627",
        "outputId": "c6ce8fd1-738c-4263-eeec-fba434669f32"
      },
      "source": [
        "# Initialize model\n",
        "BiLSTM_test_not_train_random = BiLSTM_extended(nlabels = 6, epochs=9, lr=0.003, train_parameters = False, random_embeddings = True)\n",
        "# Train model\n",
        "BiLSTM_test_not_train_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_3 = BiLSTM_test_not_train_random.eval_performance(testloader)"
      ],
      "id": "82ce1627",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5548])\n",
            " Train accuracy:  tensor([0.5622])\n",
            " Train accuracy:  tensor([0.5654])\n",
            " Train accuracy:  tensor([0.5694])\n",
            " Train accuracy:  tensor([0.5759])\n",
            " Train accuracy:  tensor([0.5813])\n",
            " Train accuracy:  tensor([0.5859])\n",
            " Train accuracy:  tensor([0.5899])\n",
            " Train accuracy:  tensor([0.5932])\n",
            " Train accuracy:  tensor([0.5962])\n",
            "Time elapsed during epoch: 735.7017230987549\n",
            " Train accuracy:  tensor([0.6277])\n",
            " Train accuracy:  tensor([0.6287])\n",
            " Train accuracy:  tensor([0.6286])\n",
            " Train accuracy:  tensor([0.6294])\n",
            " Train accuracy:  tensor([0.6310])\n",
            " Train accuracy:  tensor([0.6320])\n",
            " Train accuracy:  tensor([0.6333])\n",
            " Train accuracy:  tensor([0.6343])\n",
            " Train accuracy:  tensor([0.6353])\n",
            " Train accuracy:  tensor([0.6360])\n",
            "Time elapsed during epoch: 808.9810855388641\n",
            " Train accuracy:  tensor([0.6429])\n",
            " Train accuracy:  tensor([0.6446])\n",
            " Train accuracy:  tensor([0.6449])\n",
            " Train accuracy:  tensor([0.6449])\n",
            " Train accuracy:  tensor([0.6458])\n",
            " Train accuracy:  tensor([0.6465])\n",
            " Train accuracy:  tensor([0.6470])\n",
            " Train accuracy:  tensor([0.6474])\n",
            " Train accuracy:  tensor([0.6479])\n",
            " Train accuracy:  tensor([0.6483])\n",
            "Time elapsed during epoch: 895.6575841903687\n",
            " Train accuracy:  tensor([0.6529])\n",
            " Train accuracy:  tensor([0.6535])\n",
            " Train accuracy:  tensor([0.6532])\n",
            " Train accuracy:  tensor([0.6533])\n",
            " Train accuracy:  tensor([0.6543])\n",
            " Train accuracy:  tensor([0.6549])\n",
            " Train accuracy:  tensor([0.6554])\n",
            " Train accuracy:  tensor([0.6556])\n",
            " Train accuracy:  tensor([0.6559])\n",
            " Train accuracy:  tensor([0.6563])\n",
            "Time elapsed during epoch: 973.0729448795319\n",
            " Train accuracy:  tensor([0.6592])\n",
            " Train accuracy:  tensor([0.6592])\n",
            " Train accuracy:  tensor([0.6586])\n",
            " Train accuracy:  tensor([0.6590])\n",
            " Train accuracy:  tensor([0.6599])\n",
            " Train accuracy:  tensor([0.6603])\n",
            " Train accuracy:  tensor([0.6606])\n",
            " Train accuracy:  tensor([0.6607])\n",
            " Train accuracy:  tensor([0.6610])\n",
            " Train accuracy:  tensor([0.6612])\n",
            "Time elapsed during epoch: 998.0056293010712\n",
            " Train accuracy:  tensor([0.6635])\n",
            " Train accuracy:  tensor([0.6632])\n",
            " Train accuracy:  tensor([0.6621])\n",
            " Train accuracy:  tensor([0.6623])\n",
            " Train accuracy:  tensor([0.6632])\n",
            " Train accuracy:  tensor([0.6637])\n",
            " Train accuracy:  tensor([0.6639])\n",
            " Train accuracy:  tensor([0.6641])\n",
            " Train accuracy:  tensor([0.6643])\n",
            " Train accuracy:  tensor([0.6645])\n",
            "Time elapsed during epoch: 1132.0400004386902\n",
            " Train accuracy:  tensor([0.6660])\n",
            " Train accuracy:  tensor([0.6662])\n",
            " Train accuracy:  tensor([0.6650])\n",
            " Train accuracy:  tensor([0.6655])\n",
            " Train accuracy:  tensor([0.6661])\n",
            " Train accuracy:  tensor([0.6668])\n",
            " Train accuracy:  tensor([0.6669])\n",
            " Train accuracy:  tensor([0.6669])\n",
            " Train accuracy:  tensor([0.6671])\n",
            " Train accuracy:  tensor([0.6674])\n",
            "Time elapsed during epoch: 1212.3028342723846\n",
            " Train accuracy:  tensor([0.6684])\n",
            " Train accuracy:  tensor([0.6682])\n",
            " Train accuracy:  tensor([0.6669])\n",
            " Train accuracy:  tensor([0.6670])\n",
            " Train accuracy:  tensor([0.6675])\n",
            " Train accuracy:  tensor([0.6680])\n",
            " Train accuracy:  tensor([0.6683])\n",
            " Train accuracy:  tensor([0.6682])\n",
            " Train accuracy:  tensor([0.6686])\n",
            " Train accuracy:  tensor([0.6687])\n",
            "Time elapsed during epoch: 1298.3518133163452\n",
            " Train accuracy:  tensor([0.6686])\n",
            " Train accuracy:  tensor([0.6695])\n",
            " Train accuracy:  tensor([0.6691])\n",
            " Train accuracy:  tensor([0.6691])\n",
            " Train accuracy:  tensor([0.6696])\n",
            " Train accuracy:  tensor([0.6701])\n",
            " Train accuracy:  tensor([0.6702])\n",
            " Train accuracy:  tensor([0.6701])\n",
            " Train accuracy:  tensor([0.6703])\n",
            " Train accuracy:  tensor([0.6704])\n",
            "Time elapsed during epoch: 1303.3660054206848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "488ad568",
        "outputId": "14660288-4745-45a8-deb1-cb0c9fb10384"
      },
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_3))"
      ],
      "id": "488ad568",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.86      0.75     23507\n",
            "           1       0.50      0.11      0.17      3514\n",
            "           2       0.67      0.33      0.44     11297\n",
            "           3       0.79      0.04      0.08      1224\n",
            "           4       0.69      0.83      0.75     17472\n",
            "           5       0.71      0.36      0.48      2305\n",
            "\n",
            "    accuracy                           0.67     59319\n",
            "   macro avg       0.67      0.42      0.45     59319\n",
            "weighted avg       0.67      0.67      0.63     59319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw61yMdGLyrI",
        "outputId": "b6816e0b-16d8-4dd8-fbe3-6c9af65b45c8"
      },
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_3, labels = [1,2,3,4,5]))"
      ],
      "id": "Nw61yMdGLyrI",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.50      0.11      0.17      3514\n",
            "           2       0.67      0.33      0.44     11297\n",
            "           3       0.79      0.04      0.08      1224\n",
            "           4       0.69      0.83      0.75     17472\n",
            "           5       0.71      0.36      0.48      2305\n",
            "\n",
            "   micro avg       0.68      0.54      0.60     35812\n",
            "   macro avg       0.67      0.33      0.39     35812\n",
            "weighted avg       0.67      0.54      0.56     35812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OhSI_14L3t0",
        "outputId": "dca5f748-21b1-42b9-a449-9de7d1fb272d"
      },
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_3))"
      ],
      "id": "6OhSI_14L3t0",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[20332   157   871     1  2035   111]\n",
            " [ 1842   371   127     2  1140    32]\n",
            " [ 4522    67  3684     6  2874   144]\n",
            " [  899    20    46    53   188    18]\n",
            " [ 2294    88   599     4 14460    27]\n",
            " [ 1020    45   168     1   241   830]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12f85e24"
      },
      "source": [
        "### GloVe embeddings  +  Not training embeddings"
      ],
      "id": "12f85e24"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "498d8f5c",
        "outputId": "2b5bcb5b-864a-4ece-dc62-bcd813904ba5"
      },
      "source": [
        "# Initialize model\n",
        "BiLSTM_test_not_train_not_random = BiLSTM_extended(nlabels = 6, epochs=3, lr=0.003, train_parameters = False, random_embeddings = False)\n",
        "# Train model\n",
        "BiLSTM_test_not_train_not_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_4 = BiLSTM_test_not_train_not_random.eval_performance(testloader)"
      ],
      "id": "498d8f5c",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.6498])\n",
            " Train accuracy:  tensor([0.6686])\n",
            " Train accuracy:  tensor([0.6775])\n",
            " Train accuracy:  tensor([0.6844])\n",
            " Train accuracy:  tensor([0.6897])\n",
            " Train accuracy:  tensor([0.6942])\n",
            " Train accuracy:  tensor([0.6973])\n",
            " Train accuracy:  tensor([0.7002])\n",
            " Train accuracy:  tensor([0.7026])\n",
            " Train accuracy:  tensor([0.7045])\n",
            "Time elapsed during epoch: 732.8006269931793\n",
            " Train accuracy:  tensor([0.7253])\n",
            " Train accuracy:  tensor([0.7283])\n",
            " Train accuracy:  tensor([0.7296])\n",
            " Train accuracy:  tensor([0.7301])\n",
            " Train accuracy:  tensor([0.7313])\n",
            " Train accuracy:  tensor([0.7323])\n",
            " Train accuracy:  tensor([0.7328])\n",
            " Train accuracy:  tensor([0.7338])\n",
            " Train accuracy:  tensor([0.7347])\n",
            " Train accuracy:  tensor([0.7352])\n",
            "Time elapsed during epoch: 728.9215531349182\n",
            " Train accuracy:  tensor([0.7423])\n",
            " Train accuracy:  tensor([0.7436])\n",
            " Train accuracy:  tensor([0.7442])\n",
            " Train accuracy:  tensor([0.7447])\n",
            " Train accuracy:  tensor([0.7453])\n",
            " Train accuracy:  tensor([0.7459])\n",
            " Train accuracy:  tensor([0.7462])\n",
            " Train accuracy:  tensor([0.7467])\n",
            " Train accuracy:  tensor([0.7472])\n",
            " Train accuracy:  tensor([0.7477])\n",
            "Time elapsed during epoch: 731.9520375728607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fafdef83",
        "outputId": "98fda17c-3c1b-456b-e9ce-1c9e1dd40c3d"
      },
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_4))"
      ],
      "id": "fafdef83",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.85      0.79     23507\n",
            "           1       0.55      0.41      0.47      3514\n",
            "           2       0.68      0.55      0.61     11297\n",
            "           3       0.45      0.17      0.24      1224\n",
            "           4       0.77      0.82      0.79     17472\n",
            "           5       0.77      0.55      0.64      2305\n",
            "\n",
            "    accuracy                           0.73     59319\n",
            "   macro avg       0.66      0.56      0.59     59319\n",
            "weighted avg       0.72      0.73      0.72     59319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v5YOa8QMSB7",
        "outputId": "2008b6d1-ab2c-419d-8151-d8ccff025033"
      },
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_4, labels = [1,2,3,4,5]))"
      ],
      "id": "6v5YOa8QMSB7",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.55      0.41      0.47      3514\n",
            "           2       0.68      0.55      0.61     11297\n",
            "           3       0.45      0.17      0.24      1224\n",
            "           4       0.77      0.82      0.79     17472\n",
            "           5       0.77      0.55      0.64      2305\n",
            "\n",
            "   micro avg       0.72      0.65      0.69     35812\n",
            "   macro avg       0.65      0.50      0.55     35812\n",
            "weighted avg       0.71      0.65      0.67     35812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XL7uluOGn-d",
        "outputId": "cd0ba5b3-7e76-4377-b3b4-6b078d721a40"
      },
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_4))"
      ],
      "id": "3XL7uluOGn-d",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[19894   496  1386    98  1475   158]\n",
            " [ 1074  1442   196    44   719    39]\n",
            " [ 2928   182  6167    42  1872   106]\n",
            " [  691    64    96   205   147    21]\n",
            " [ 1681   356  1022    46 14312    55]\n",
            " [  656    68   145    18   154  1264]]\n"
          ]
        }
      ]
    }
  ]
}