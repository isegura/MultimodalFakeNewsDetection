{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "479fe902"
      },
      "source": [
        "# CNN\n",
        "\n",
        "We start by importing the libraries."
      ],
      "id": "479fe902"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33bc9936",
        "outputId": "68899d42-4bc8-466d-c5be-1cfeb9caea08"
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import spacy\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, SpatialDropout1D, Conv1D, MaxPooling1D, GRU, BatchNormalization\n",
        "from tensorflow.keras.layers import Input, Bidirectional, GlobalAveragePooling1D, concatenate, LeakyReLU, GlobalMaxPooling1D, Flatten\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "33bc9936",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeGlZfzKab2Y",
        "outputId": "e3387300-c847-40b4-d839-f220059bdaa6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "jeGlZfzKab2Y",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5df1b35a"
      },
      "source": [
        "We also import the data."
      ],
      "id": "5df1b35a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a27cd18f"
      },
      "source": [
        "# Train data \n",
        "traindata_all = pd.read_csv('/content/drive/MyDrive/TFM/multimodal_train.tsv',sep='\\t')\n",
        "# Validation data \n",
        "validata_all = pd.read_csv('/content/drive/MyDrive/TFM/multimodal_validate.tsv',sep='\\t')\n",
        "# Test data \n",
        "testdata_all = pd.read_csv('/content/drive/MyDrive/TFM/multimodal_test_public.tsv',sep='\\t')"
      ],
      "id": "a27cd18f",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56eed200"
      },
      "source": [
        "We select a subset of the dataframe with no missing values in the 'clean_title' column."
      ],
      "id": "56eed200"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3574cfc"
      },
      "source": [
        "# Train data with no missing values\n",
        "train_data = traindata_all[traindata_all['clean_title'].notnull().to_numpy()]\n",
        "# Validation data with no missing values\n",
        "valid_data = validata_all[validata_all['clean_title'].notnull().to_numpy()]\n",
        "# Test data with no missing values\n",
        "test_data = testdata_all[testdata_all['clean_title'].notnull().to_numpy()]"
      ],
      "id": "e3574cfc",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d62f80a"
      },
      "source": [
        "And we separate the datasets into the texts and the labels"
      ],
      "id": "5d62f80a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9149859d"
      },
      "source": [
        "## Train data\n",
        "train_news = list(train_data['clean_title'])\n",
        "train_labels = list(train_data['6_way_label'])\n",
        "## Valid data\n",
        "valid_news = list(valid_data['clean_title'])\n",
        "valid_labels = list(valid_data['6_way_label'])\n",
        "## Test data\n",
        "test_news = list(test_data['clean_title'])\n",
        "test_labels = list(test_data['6_way_label'])"
      ],
      "id": "9149859d",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a444c553"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We define a function to preprocess the data. We remove punctuations and numbers and also multiple spaces."
      ],
      "id": "a444c553"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2571f879"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "id": "2571f879",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab17f801"
      },
      "source": [
        "# Remove puntuations and numbers and multiple spaces\n",
        "\n",
        "train_news_clean_1 = []\n",
        "valid_news_clean_1 = []\n",
        "test_news_clean_1 = []\n",
        "# Train\n",
        "for new in train_news:\n",
        "    train_news_clean_1.append(preprocess_text(new))\n",
        "# Validation\n",
        "for new in valid_news:\n",
        "    valid_news_clean_1.append(preprocess_text(new))\n",
        "# Test\n",
        "for new in test_news:\n",
        "    test_news_clean_1.append(preprocess_text(new))"
      ],
      "id": "ab17f801",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f1903e8"
      },
      "source": [
        "Now  we remove stop words and perform lemmatization. We define the function to do that."
      ],
      "id": "3f1903e8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79086ac8"
      },
      "source": [
        "# Initialize  lemmatizer and  stop_words\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Function to remove stopwords and perform lemmatization\n",
        "def remove_stopwords_lem(text):\n",
        "    text = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    # Lematization\n",
        "    lemmatized_text = []\n",
        "    for word in text:\n",
        "        word1 = lemmatizer.lemmatize(word, pos = \"n\")\n",
        "        word2 = lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "        word3 = lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "        lemmatized_text.append(word3)\n",
        "        \n",
        "    text_done = ' '.join(lemmatized_text)\n",
        "    return text_done"
      ],
      "id": "79086ac8",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16ef9e76"
      },
      "source": [
        "And now perform stop-words removal and lemmatization."
      ],
      "id": "16ef9e76"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e81981a6"
      },
      "source": [
        "# Stop-words removal and lemmatization\n",
        "train_stwrd_lem = []\n",
        "valid_stwrd_lem = []\n",
        "test_stwrd_lem = []\n",
        "\n",
        "# Train\n",
        "for new in train_news_clean_1:\n",
        "    train_stwrd_lem.append(remove_stopwords_lem(new))\n",
        "# Validation\n",
        "for new in valid_news_clean_1:\n",
        "    valid_stwrd_lem.append(remove_stopwords_lem(new))\n",
        "# Test\n",
        "for new in test_news_clean_1:\n",
        "    test_stwrd_lem.append(remove_stopwords_lem(new))"
      ],
      "id": "e81981a6",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eca516c"
      },
      "source": [
        "We train a tokenizer using all the documents and we use the learned vocabulary in order to transform texts into sequences of ID's."
      ],
      "id": "5eca516c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21ed842a"
      },
      "source": [
        "news_all = train_stwrd_lem + valid_stwrd_lem + test_stwrd_lem\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 128022)\n",
        "tokenizer.fit_on_texts(news_all)\n",
        "\n",
        "# Tokenize news\n",
        "\n",
        "# Train\n",
        "train_tokenized = tokenizer.texts_to_sequences(train_stwrd_lem)\n",
        "# Validation\n",
        "valid_tokenized = tokenizer.texts_to_sequences(valid_stwrd_lem)\n",
        "# Test\n",
        "test_tokenized = tokenizer.texts_to_sequences(test_stwrd_lem)"
      ],
      "id": "21ed842a",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS84b4HnhlEo"
      },
      "source": [
        "Obtain the vocabulary length"
      ],
      "id": "cS84b4HnhlEo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPFdr3kQhp_B",
        "outputId": "482b9ee1-1ad7-4a53-e13d-163330fd45bb"
      },
      "source": [
        "print(\"Vocabulary length: \", len(tokenizer.word_index))"
      ],
      "id": "UPFdr3kQhp_B",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary length:  117129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259c25be"
      },
      "source": [
        "Now we pad the sequences of numbers generated by the tokenizer. But firstly we check how many sequences are shorter than a given length. We start by defining a function that counts the length of each sequence."
      ],
      "id": "259c25be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4f507be"
      },
      "source": [
        "# Function to count the lenght of each sequence\n",
        "def length_squences(data):\n",
        "    lengths = []\n",
        "    for i in range(len(data)):\n",
        "        lengths.append(len(data[i]))\n",
        "    return lengths"
      ],
      "id": "a4f507be",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1df362bd"
      },
      "source": [
        "Now we count the lenghts of all the sequences in the training, validation and test set and we check what % of sequences in the train, validation and test sets are smaller than a given length when they are tokenized."
      ],
      "id": "1df362bd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k_WvwCqkW5K",
        "outputId": "0aee56e7-733c-4398-ffd3-4ca2c6036ef1"
      },
      "source": [
        "length = [10, 15, 20, 25]\n",
        "\n",
        "# Train set\n",
        "lengths_train = np.array(length_squences(train_tokenized))\n",
        "perc_length_train = []\n",
        "for lgth in length:\n",
        "   perc_length_train.append( sum(lengths_train < lgth)/len(lengths_train)*100)\n",
        "print(\"Pertentages (train):\", perc_length_train)\n",
        "\n",
        "# Validation set \n",
        "lengths_valid = np.array(length_squences(valid_tokenized))\n",
        "perc_length_valid = []\n",
        "for lgth in length:\n",
        "   perc_length_valid.append( sum(lengths_valid < lgth)/len(lengths_valid)*100)\n",
        "print(\"Pertentages (validation):\", perc_length_valid)\n",
        "\n",
        "# Test set\n",
        "lengths_test = np.array(length_squences(test_tokenized))\n",
        "perc_length_test = []\n",
        "for lgth in length:\n",
        "   perc_length_test.append( sum(lengths_test < lgth)/len(lengths_test)*100)\n",
        "print(\"Pertentages (test):\", perc_length_test)"
      ],
      "id": "8k_WvwCqkW5K",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pertentages (train) [91.42251773049645, 98.45230496453901, 99.49361702127659, 99.81382978723404]\n",
            "Pertentages (validation) [91.54898722658487, 98.43449833170436, 99.49614101311045, 99.80957837619225]\n",
            "Pertentages (test) [91.31812741280197, 98.44063453530909, 99.50268885180128, 99.8179335457442]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgG1riOjlxNb"
      },
      "source": [
        "We plot the results."
      ],
      "id": "JgG1riOjlxNb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "UoF5Nrlll08D",
        "outputId": "fe39d5c3-ed1a-4f60-c746-53962dcb6cca"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# create dataset\n",
        "bars = [91.42, 91.55, 91.32, 98.45, 98.43, 98.44, 99.49, 99.50, 99.50, 99.81, 99.81, 99.82]\n",
        "labels = ['< 10', '< 15', '< 20', '< 25']\n",
        "\n",
        "x_pos_bars = [1, 2, 3, 6, 7, 8, 11, 12, 13, 16, 17, 18]\n",
        "x_pos_labels = [2, 7, 12, 17]\n",
        "\n",
        "# Colors and mapping to values\n",
        "colors = ['b', 'g', 'y']\n",
        "colors_values = {'Train':'b', 'Validation':'g', 'Test': 'y'}    \n",
        "\n",
        "labels2 = list(colors_values.keys())\n",
        "handles = [plt.Rectangle((0,0),1,1, color=colors_values[label]) for label in labels2]\n",
        "# Make the plot\n",
        "plt.bar(x_pos_bars, bars,color = colors, label = colors_values)\n",
        "\n",
        "# Create names on the x-axis\n",
        "plt.xticks(x_pos_labels, labels)\n",
        "plt.legend(handles, labels2)\n",
        "plt.ylim([0, 130])\n",
        "#plt.xlabel('Metrics')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title('% of texts smaller than a given length')\n",
        "plt.show()"
      ],
      "id": "UoF5Nrlll08D",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "655078ba"
      },
      "source": [
        "As we can see almost all news are smaller than **15** in length when tokenized so choosing this lenght to pad/truncate the tokenized news will not eliminate any information from the news in almost any case."
      ],
      "id": "655078ba"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "999ccdba"
      },
      "source": [
        "# Pad/truncate the tokenized news\n",
        "\n",
        "# Train\n",
        "train_tokenized_pad = pad_sequences(train_tokenized, maxlen = 15, truncating = 'pre', padding = 'pre')\n",
        "# Validation\n",
        "valid_tokenized_pad = pad_sequences(valid_tokenized, maxlen = 15, truncating = 'pre', padding = 'pre')\n",
        "# Test\n",
        "test_tokenized_pad = pad_sequences(test_tokenized, maxlen = 15, truncating = 'pre', padding = 'pre')"
      ],
      "id": "999ccdba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c40705c"
      },
      "source": [
        "In order to use the data with torch we have to transform the arrays into dataloader objects but first they need to be transformed into tensors."
      ],
      "id": "3c40705c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cdbc432"
      },
      "source": [
        "# Transform data arrays into tensors\n",
        "\n",
        "# Train\n",
        "train_tensor = torch.Tensor(train_tokenized_pad).int()\n",
        "# Validation\n",
        "valid_tensor = torch.Tensor(valid_tokenized_pad).int()\n",
        "# Test\n",
        "test_tensor =  torch.Tensor(test_tokenized_pad).int()\n",
        "\n",
        "# Tranform tensors into data loader objects\n",
        "\n",
        "# Train\n",
        "train_set = TensorDataset(train_tensor, torch.Tensor(np.array(train_labels)))\n",
        "trainloader = DataLoader(train_set, batch_size=60)\n",
        "# Validation\n",
        "valid_set = TensorDataset(valid_tensor, torch.Tensor(np.array(valid_labels)))\n",
        "validloader = DataLoader(valid_set, batch_size=60)\n",
        "# Test\n",
        "test_set = TensorDataset(test_tensor, torch.Tensor(np.array(test_labels)))\n",
        "testloader =  DataLoader(test_set, batch_size=60)"
      ],
      "id": "0cdbc432",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781b4da2"
      },
      "source": [
        "### Word embeddings\n",
        "\n",
        "We will try different word embeddings and select the one which performs better. We create two functions: one to load the word embeddings and the other to create the embedding matrix."
      ],
      "id": "781b4da2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31ea6341"
      },
      "source": [
        "# Function to load the word embeddings\n",
        "\n",
        "def load_embedd(filename):\n",
        "    words = []\n",
        "    vectors = []\n",
        "    file = open(filename,'r', encoding=\"utf8\")\n",
        "    for line in file.readlines():\n",
        "       row = line.split(' ')\n",
        "       vocab = row[0]\n",
        "       embd = row[1:len(row)]\n",
        "       embd[-1] = embd[-1].rstrip()\n",
        "       embd = list(map(float,embd)) # convert string to float\n",
        "       words.append(vocab)\n",
        "       vectors.append(embd)\n",
        "    file.close()\n",
        "    return words,vectors"
      ],
      "id": "31ea6341",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc4aef6f"
      },
      "source": [
        "Function to create the embedding matrix."
      ],
      "id": "bc4aef6f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa5f5128"
      },
      "source": [
        "# Function to create the embedding matrix\n",
        "\n",
        "def embed_matx(word_index, vocab, embeddings, length_vocab, length_embedding):\n",
        "    embedding_matrix = np.zeros((length_vocab +1, length_embedding))\n",
        "    for word, i in word_index.items():\n",
        "        if word in vocab:\n",
        "            idx = vocab.index(word)\n",
        "            vector =  embeddings[idx]\n",
        "            embedding_matrix[i] = vector\n",
        "        if i == length_vocab:\n",
        "            break\n",
        "    return embedding_matrix"
      ],
      "id": "aa5f5128",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "991cf6d2"
      },
      "source": [
        "#### Glove (300 d)\n",
        "\n",
        "We use GloVe embeddings of dimension 300."
      ],
      "id": "991cf6d2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dca6f8a7"
      },
      "source": [
        "vocab_gv_300, vectors_gv_300 = load_embedd(filename = \"/content/drive/MyDrive/TFM/glove.6B.300d.txt\")"
      ],
      "id": "dca6f8a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b14586f"
      },
      "source": [
        "Now we create the embbeding matrix."
      ],
      "id": "8b14586f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c454f83"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "# Embedding matrix\n",
        "embedding_matrix_gv_300 = embed_matx(word_index = word_index, vocab = vocab_gv_300, embeddings = vectors_gv_300, \n",
        "                             length_vocab = 117129, length_embedding = 300)"
      ],
      "id": "3c454f83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ceb5c6"
      },
      "source": [
        "## Models\n",
        "\n",
        "First we define the CNN that we will use."
      ],
      "id": "82ceb5c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90be399d"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, nlabels, train_parameters = True, random_embeddings = True): \n",
        "        super().__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings =117130, embedding_dim = 300)\n",
        "        \n",
        "        if random_embeddings == True:\n",
        "            self.embedding.weight = nn.Parameter(torch.rand(117130, 300), requires_grad = train_parameters)\n",
        "        else:\n",
        "            self.embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix_gv_300), requires_grad = train_parameters)\n",
        "            \n",
        "        # Filters for the CNN    \n",
        "        self.filter_sizes = [2,3,4,5]\n",
        "        self.num_filters = 50\n",
        "        \n",
        "        # Concolutional layers\n",
        "        self.convs_concat = nn.ModuleList([nn.Conv2d(1, self.num_filters, (K, 300)) for K in self.filter_sizes])\n",
        "        \n",
        "        # Linear layers\n",
        "        self.linear1 = nn.Linear(200,128)\n",
        "  \n",
        "        self.linear2 = nn.Linear(128,nlabels)\n",
        "    \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        x = self.embedding(x)\n",
        "        # Unsqueeze\n",
        "        x = x.unsqueeze(1)\n",
        "        # Convolution\n",
        "        x = [F.relu(conv(x.float())).squeeze(3) for conv in self.convs_concat]\n",
        "        # Max-pooling\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] \n",
        "        x = torch.cat(x, 1)\n",
        "        # Linear layers\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.logsoftmax(x) \n",
        "        return x"
      ],
      "id": "90be399d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86f2a4c8"
      },
      "source": [
        "class CNN_extended(CNN):\n",
        "    \n",
        "    def __init__(self,nlabels, train_parameters, random_embeddings, epochs=100, lr=0.001):\n",
        "        \n",
        "        super().__init__(nlabels, train_parameters, random_embeddings)  \n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        \n",
        "        self.epochs = epochs\n",
        "        \n",
        "        self.criterion = nn.NLLLoss()              \n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "\n",
        "        self.valid_loss_during_training = []\n",
        "        \n",
        "    def trainloop(self,trainloader, validloader):\n",
        "        \n",
        "        \n",
        "        # Optimization Loop\n",
        "        \n",
        "        for e in range(int(self.epochs)):\n",
        "\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Random data permutation at each epoch\n",
        "            \n",
        "            running_loss = 0.\n",
        "            \n",
        "            i = 0\n",
        "            \n",
        "            length = 0\n",
        "            \n",
        "            accuracies = []\n",
        "            \n",
        "            for news, labels in trainloader:             \n",
        "        \n",
        "                self.optim.zero_grad()  # Reset gradients\n",
        "            \n",
        "                out = self.forward(news.int())\n",
        "\n",
        "                loss = self.criterion(out,labels.long())\n",
        "                \n",
        "                loss.backward()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                self.optim.step()\n",
        "                \n",
        "                top_p, top_class = out.topk(1, dim=1)\n",
        "                \n",
        "                equals = (top_class == labels.view(news.shape[0], 1))\n",
        "                \n",
        "                length += news.shape[0]\n",
        "                \n",
        "                accuracies.append(sum(equals)) \n",
        "                \n",
        "                accuracy = sum(accuracies)/length\n",
        "                \n",
        "                i += 1\n",
        "                \n",
        "                if i%1000 == 0:\n",
        "                    print(\" Train accuracy: \", accuracy)\n",
        "                \n",
        "            self.loss_during_training.append(running_loss/len(trainloader))\n",
        "\n",
        "            # Validation Loss\n",
        "            \n",
        "            with torch.no_grad():            \n",
        "                \n",
        "                running_loss = 0.\n",
        "                \n",
        "                i = 0\n",
        "                \n",
        "                length = 0\n",
        "                \n",
        "                accuracies = []\n",
        "                \n",
        "                for news,labels in validloader:\n",
        "                    \n",
        "                    out = self.forward(news.int())\n",
        "\n",
        "                    loss = self.criterion(out,labels.long())\n",
        "\n",
        "                    running_loss += loss.item()   \n",
        "                    \n",
        "                    top_p, top_class = out.topk(1, dim=1)\n",
        "                \n",
        "                    equals = (top_class == labels.view(news.shape[0], 1))\n",
        "                \n",
        "                    length += news.shape[0]\n",
        "                \n",
        "                    accuracies.append(sum(equals)) \n",
        "                \n",
        "                    accuracy = sum(accuracies)/length\n",
        "                    \n",
        "                print(\" Validation accuracy: \", accuracy)                    \n",
        "                      \n",
        "                self.valid_loss_during_training.append(running_loss/len(validloader))\n",
        "\n",
        "            if(e % 1 == 0): # Every 10 epochs\n",
        "\n",
        "                print(\"Training loss after %d epochs: %f\" \n",
        "                      %(e,self.loss_during_training[-1]), \"Validation loss after %d epochs: %f\" %(e,self.valid_loss_during_training[-1]),\n",
        "                      \"Time per epoch: %f seconds\"%(time.time() - start_time))"
      ],
      "id": "86f2a4c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa003f65"
      },
      "source": [
        "We train the CNN in **4** different scenarios:\n",
        "\n",
        "  - **Random embeddings** + **Training embeddings**\n",
        "  - **GloVe embeddings** + **Training embeddings**\n",
        "  - **Random embeddings** + **Not training embeddings**\n",
        "  - **GloVe embeddings** + **Not training embeddings**\n",
        " \n",
        "We train the model for several epochs. This is done in order to select the optimal number of epochs, which occurs when the validation loss stops decreasing and starts increasing (Early Stopping). Sometimes the validation loss starts to increase after very few epochs. In those cases we stop the training prematurely since we already know the optimal number of epochs (this is what causes the \"KeyboardInterrupt\" message in some of the results which can be seen below)."
      ],
      "id": "fa003f65"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "358c8b67"
      },
      "source": [
        "### Random embeddings  + Training embeddings"
      ],
      "id": "358c8b67"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "87b6b117",
        "outputId": "14176d42-42b8-40f0-966f-8df3f9b00be4"
      },
      "source": [
        "# Initialize model\n",
        "CNN_train_random = CNN_extended(nlabels = 6, epochs=5, lr=0.003, train_parameters = True, random_embeddings = True)\n",
        "# Train model\n",
        "CNN_train_random.trainloop(trainloader, validloader)"
      ],
      "id": "87b6b117",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5957])\n",
            " Train accuracy:  tensor([0.6299])\n",
            " Train accuracy:  tensor([0.6476])\n",
            " Train accuracy:  tensor([0.6581])\n",
            " Train accuracy:  tensor([0.6661])\n",
            " Train accuracy:  tensor([0.6726])\n",
            " Train accuracy:  tensor([0.6772])\n",
            " Train accuracy:  tensor([0.6813])\n",
            " Train accuracy:  tensor([0.6849])\n",
            " Validation accuracy:  tensor([0.7139])\n",
            "Training loss after 0 epochs: 0.881487 Validation loss after 0 epochs: 0.812628 Time per epoch: 6060.227753 seconds\n",
            " Train accuracy:  tensor([0.7296])\n",
            " Train accuracy:  tensor([0.7350])\n",
            " Train accuracy:  tensor([0.7385])\n",
            " Train accuracy:  tensor([0.7408])\n",
            " Train accuracy:  tensor([0.7430])\n",
            " Train accuracy:  tensor([0.7449])\n",
            " Train accuracy:  tensor([0.7463])\n",
            " Train accuracy:  tensor([0.7477])\n",
            " Train accuracy:  tensor([0.7493])\n",
            " Validation accuracy:  tensor([0.7138])\n",
            "Training loss after 1 epochs: 0.711673 Validation loss after 1 epochs: 0.837501 Time per epoch: 7207.740807 seconds\n",
            " Train accuracy:  tensor([0.7730])\n",
            " Train accuracy:  tensor([0.7778])\n",
            " Train accuracy:  tensor([0.7797])\n",
            " Train accuracy:  tensor([0.7817])\n",
            " Train accuracy:  tensor([0.7836])\n",
            " Train accuracy:  tensor([0.7848])\n",
            " Train accuracy:  tensor([0.7857])\n",
            " Train accuracy:  tensor([0.7864])\n",
            " Train accuracy:  tensor([0.7874])\n",
            " Validation accuracy:  tensor([0.7122])\n",
            "Training loss after 2 epochs: 0.612557 Validation loss after 2 epochs: 0.873512 Time per epoch: 7261.114381 seconds\n",
            " Train accuracy:  tensor([0.8048])\n",
            " Train accuracy:  tensor([0.8084])\n",
            " Train accuracy:  tensor([0.8091])\n",
            " Train accuracy:  tensor([0.8099])\n",
            " Train accuracy:  tensor([0.8111])\n",
            " Train accuracy:  tensor([0.8118])\n",
            " Train accuracy:  tensor([0.8123])\n",
            " Train accuracy:  tensor([0.8127])\n",
            " Train accuracy:  tensor([0.8133])\n",
            " Validation accuracy:  tensor([0.7077])\n",
            "Training loss after 3 epochs: 0.542114 Validation loss after 3 epochs: 0.921283 Time per epoch: 7266.223625 seconds\n",
            " Train accuracy:  tensor([0.8261])\n",
            " Train accuracy:  tensor([0.8286])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-5a972d7112d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mCNN_train_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mCNN_train_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-126a2a95584d>\u001b[0m in \u001b[0;36mtrainloop\u001b[0;34m(self, trainloader, validloader)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "525d5d4a"
      },
      "source": [
        "### GloVe embeddings  + Training embeddings"
      ],
      "id": "525d5d4a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "e0ab5a99",
        "outputId": "609c83a4-46e1-4a68-f061-523d363e83a8"
      },
      "source": [
        "# Initialize model\n",
        "CNN_train_not_random = CNN_extended(nlabels = 6, epochs=4, lr=0.003, train_parameters = True, random_embeddings = False)\n",
        "# Train model\n",
        "CNN_train_not_random.trainloop(trainloader, validloader)"
      ],
      "id": "e0ab5a99",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.6674])\n",
            " Train accuracy:  tensor([0.6835])\n",
            " Train accuracy:  tensor([0.6923])\n",
            " Train accuracy:  tensor([0.6978])\n",
            " Train accuracy:  tensor([0.7029])\n",
            " Train accuracy:  tensor([0.7071])\n",
            " Train accuracy:  tensor([0.7100])\n",
            " Train accuracy:  tensor([0.7125])\n",
            " Train accuracy:  tensor([0.7149])\n",
            " Validation accuracy:  tensor([0.7311])\n",
            "Training loss after 0 epochs: 0.805835 Validation loss after 0 epochs: 0.765596 Time per epoch: 9030.680334 seconds\n",
            " Train accuracy:  tensor([0.7588])\n",
            " Train accuracy:  tensor([0.7662])\n",
            " Train accuracy:  tensor([0.7695])\n",
            " Train accuracy:  tensor([0.7721])\n",
            " Train accuracy:  tensor([0.7750])\n",
            " Train accuracy:  tensor([0.7773])\n",
            " Train accuracy:  tensor([0.7789])\n",
            " Train accuracy:  tensor([0.7803])\n",
            " Train accuracy:  tensor([0.7818])\n",
            " Validation accuracy:  tensor([0.7244])\n",
            "Training loss after 1 epochs: 0.623853 Validation loss after 1 epochs: 0.826593 Time per epoch: 10139.675975 seconds\n",
            " Train accuracy:  tensor([0.8123])\n",
            " Train accuracy:  tensor([0.8189])\n",
            " Train accuracy:  tensor([0.8214])\n",
            " Train accuracy:  tensor([0.8225])\n",
            " Train accuracy:  tensor([0.8248])\n",
            " Train accuracy:  tensor([0.8263])\n",
            " Train accuracy:  tensor([0.8273])\n",
            " Train accuracy:  tensor([0.8282])\n",
            " Train accuracy:  tensor([0.8291])\n",
            " Validation accuracy:  tensor([0.7171])\n",
            "Training loss after 2 epochs: 0.492774 Validation loss after 2 epochs: 0.922204 Time per epoch: 10200.415422 seconds\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8dd6dc9b5e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mCNN_train_not_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mCNN_train_not_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-126a2a95584d>\u001b[0m in \u001b[0;36mtrainloop\u001b[0;34m(self, trainloader, validloader)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37d85d1"
      },
      "source": [
        "### Random embeddings  + Not training embeddings"
      ],
      "id": "f37d85d1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74c7d4f8",
        "outputId": "f60aa807-e4cd-47a0-bc14-d6ad6b385c42"
      },
      "source": [
        "# Initialize model\n",
        "CNN_not_train_random = CNN_extended(nlabels = 6, epochs=9, lr=0.003, train_parameters = False, random_embeddings = True)\n",
        "# Train model\n",
        "CNN_not_train_random.trainloop(trainloader, validloader)"
      ],
      "id": "74c7d4f8",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5501])\n",
            " Train accuracy:  tensor([0.5608])\n",
            " Train accuracy:  tensor([0.5671])\n",
            " Train accuracy:  tensor([0.5729])\n",
            " Train accuracy:  tensor([0.5778])\n",
            " Train accuracy:  tensor([0.5820])\n",
            " Train accuracy:  tensor([0.5851])\n",
            " Train accuracy:  tensor([0.5879])\n",
            " Train accuracy:  tensor([0.5903])\n",
            " Validation accuracy:  tensor([0.6015])\n",
            "Training loss after 0 epochs: 1.121780 Validation loss after 0 epochs: 1.108112 Time per epoch: 635.669165 seconds\n",
            " Train accuracy:  tensor([0.6119])\n",
            " Train accuracy:  tensor([0.6135])\n",
            " Train accuracy:  tensor([0.6132])\n",
            " Train accuracy:  tensor([0.6132])\n",
            " Train accuracy:  tensor([0.6138])\n",
            " Train accuracy:  tensor([0.6145])\n",
            " Train accuracy:  tensor([0.6152])\n",
            " Train accuracy:  tensor([0.6157])\n",
            " Train accuracy:  tensor([0.6164])\n",
            " Validation accuracy:  tensor([0.6112])\n",
            "Training loss after 1 epochs: 1.062122 Validation loss after 1 epochs: 1.081675 Time per epoch: 637.249443 seconds\n",
            " Train accuracy:  tensor([0.6229])\n",
            " Train accuracy:  tensor([0.6240])\n",
            " Train accuracy:  tensor([0.6228])\n",
            " Train accuracy:  tensor([0.6233])\n",
            " Train accuracy:  tensor([0.6236])\n",
            " Train accuracy:  tensor([0.6239])\n",
            " Train accuracy:  tensor([0.6244])\n",
            " Train accuracy:  tensor([0.6247])\n",
            " Train accuracy:  tensor([0.6253])\n",
            " Validation accuracy:  tensor([0.6222])\n",
            "Training loss after 2 epochs: 1.042205 Validation loss after 2 epochs: 1.062248 Time per epoch: 634.489743 seconds\n",
            " Train accuracy:  tensor([0.6297])\n",
            " Train accuracy:  tensor([0.6298])\n",
            " Train accuracy:  tensor([0.6285])\n",
            " Train accuracy:  tensor([0.6284])\n",
            " Train accuracy:  tensor([0.6288])\n",
            " Train accuracy:  tensor([0.6292])\n",
            " Train accuracy:  tensor([0.6296])\n",
            " Train accuracy:  tensor([0.6299])\n",
            " Train accuracy:  tensor([0.6303])\n",
            " Validation accuracy:  tensor([0.6246])\n",
            "Training loss after 3 epochs: 1.030901 Validation loss after 3 epochs: 1.055378 Time per epoch: 628.260647 seconds\n",
            " Train accuracy:  tensor([0.6310])\n",
            " Train accuracy:  tensor([0.6324])\n",
            " Train accuracy:  tensor([0.6314])\n",
            " Train accuracy:  tensor([0.6315])\n",
            " Train accuracy:  tensor([0.6318])\n",
            " Train accuracy:  tensor([0.6322])\n",
            " Train accuracy:  tensor([0.6325])\n",
            " Train accuracy:  tensor([0.6327])\n",
            " Train accuracy:  tensor([0.6332])\n",
            " Validation accuracy:  tensor([0.6273])\n",
            "Training loss after 4 epochs: 1.024776 Validation loss after 4 epochs: 1.046851 Time per epoch: 651.401029 seconds\n",
            " Train accuracy:  tensor([0.6340])\n",
            " Train accuracy:  tensor([0.6344])\n",
            " Train accuracy:  tensor([0.6337])\n",
            " Train accuracy:  tensor([0.6336])\n",
            " Train accuracy:  tensor([0.6340])\n",
            " Train accuracy:  tensor([0.6342])\n",
            " Train accuracy:  tensor([0.6344])\n",
            " Train accuracy:  tensor([0.6345])\n",
            " Train accuracy:  tensor([0.6350])\n",
            " Validation accuracy:  tensor([0.6323])\n",
            "Training loss after 5 epochs: 1.019743 Validation loss after 5 epochs: 1.033113 Time per epoch: 677.647488 seconds\n",
            " Train accuracy:  tensor([0.6350])\n",
            " Train accuracy:  tensor([0.6362])\n",
            " Train accuracy:  tensor([0.6354])\n",
            " Train accuracy:  tensor([0.6353])\n",
            " Train accuracy:  tensor([0.6357])\n",
            " Train accuracy:  tensor([0.6358])\n",
            " Train accuracy:  tensor([0.6362])\n",
            " Train accuracy:  tensor([0.6364])\n",
            " Train accuracy:  tensor([0.6368])\n",
            " Validation accuracy:  tensor([0.6341])\n",
            "Training loss after 6 epochs: 1.015542 Validation loss after 6 epochs: 1.027596 Time per epoch: 692.991172 seconds\n",
            " Train accuracy:  tensor([0.6381])\n",
            " Train accuracy:  tensor([0.6381])\n",
            " Train accuracy:  tensor([0.6371])\n",
            " Train accuracy:  tensor([0.6371])\n",
            " Train accuracy:  tensor([0.6376])\n",
            " Train accuracy:  tensor([0.6378])\n",
            " Train accuracy:  tensor([0.6381])\n",
            " Train accuracy:  tensor([0.6383])\n",
            " Train accuracy:  tensor([0.6387])\n",
            " Validation accuracy:  tensor([0.6345])\n",
            "Training loss after 7 epochs: 1.011294 Validation loss after 7 epochs: 1.024060 Time per epoch: 833.903002 seconds\n",
            " Train accuracy:  tensor([0.6385])\n",
            " Train accuracy:  tensor([0.6386])\n",
            " Train accuracy:  tensor([0.6384])\n",
            " Train accuracy:  tensor([0.6383])\n",
            " Train accuracy:  tensor([0.6387])\n",
            " Train accuracy:  tensor([0.6390])\n",
            " Train accuracy:  tensor([0.6393])\n",
            " Train accuracy:  tensor([0.6396])\n",
            " Train accuracy:  tensor([0.6399])\n",
            " Validation accuracy:  tensor([0.6362])\n",
            "Training loss after 8 epochs: 1.007963 Validation loss after 8 epochs: 1.019992 Time per epoch: 895.114388 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dbc4f66"
      },
      "source": [
        "### GloVe embeddings  + Not training embeddings"
      ],
      "id": "7dbc4f66"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5f16202",
        "outputId": "1d3dd5fe-b663-4c28-cf89-c6d63890431e"
      },
      "source": [
        "# Initialize model\n",
        "CNN_not_train_not_random = CNN_extended(nlabels = 6, epochs=9, lr=0.003, train_parameters = False, random_embeddings = False)\n",
        "# Train model\n",
        "CNN_not_train_not_random.trainloop(trainloader, validloader)"
      ],
      "id": "d5f16202",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.6494])\n",
            " Train accuracy:  tensor([0.6620])\n",
            " Train accuracy:  tensor([0.6693])\n",
            " Train accuracy:  tensor([0.6744])\n",
            " Train accuracy:  tensor([0.6784])\n",
            " Train accuracy:  tensor([0.6814])\n",
            " Train accuracy:  tensor([0.6839])\n",
            " Train accuracy:  tensor([0.6862])\n",
            " Train accuracy:  tensor([0.6880])\n",
            " Validation accuracy:  tensor([0.7028])\n",
            "Training loss after 0 epochs: 0.875074 Validation loss after 0 epochs: 0.833539 Time per epoch: 662.245848 seconds\n",
            " Train accuracy:  tensor([0.7074])\n",
            " Train accuracy:  tensor([0.7092])\n",
            " Train accuracy:  tensor([0.7107])\n",
            " Train accuracy:  tensor([0.7116])\n",
            " Train accuracy:  tensor([0.7131])\n",
            " Train accuracy:  tensor([0.7139])\n",
            " Train accuracy:  tensor([0.7148])\n",
            " Train accuracy:  tensor([0.7158])\n",
            " Train accuracy:  tensor([0.7167])\n",
            " Validation accuracy:  tensor([0.7108])\n",
            "Training loss after 1 epochs: 0.798680 Validation loss after 1 epochs: 0.823584 Time per epoch: 656.923089 seconds\n",
            " Train accuracy:  tensor([0.7263])\n",
            " Train accuracy:  tensor([0.7267])\n",
            " Train accuracy:  tensor([0.7273])\n",
            " Train accuracy:  tensor([0.7278])\n",
            " Train accuracy:  tensor([0.7287])\n",
            " Train accuracy:  tensor([0.7295])\n",
            " Train accuracy:  tensor([0.7302])\n",
            " Train accuracy:  tensor([0.7309])\n",
            " Train accuracy:  tensor([0.7317])\n",
            " Validation accuracy:  tensor([0.7079])\n",
            "Training loss after 2 epochs: 0.757470 Validation loss after 2 epochs: 0.836110 Time per epoch: 651.707693 seconds\n",
            " Train accuracy:  tensor([0.7377])\n",
            " Train accuracy:  tensor([0.7383])\n",
            " Train accuracy:  tensor([0.7390])\n",
            " Train accuracy:  tensor([0.7394])\n",
            " Train accuracy:  tensor([0.7402])\n",
            " Train accuracy:  tensor([0.7407])\n",
            " Train accuracy:  tensor([0.7412])\n",
            " Train accuracy:  tensor([0.7418])\n",
            " Train accuracy:  tensor([0.7425])\n",
            " Validation accuracy:  tensor([0.7079])\n",
            "Training loss after 3 epochs: 0.727899 Validation loss after 3 epochs: 0.854106 Time per epoch: 652.215284 seconds\n",
            " Train accuracy:  tensor([0.7452])\n",
            " Train accuracy:  tensor([0.7470])\n",
            " Train accuracy:  tensor([0.7471])\n",
            " Train accuracy:  tensor([0.7480])\n",
            " Train accuracy:  tensor([0.7490])\n",
            " Train accuracy:  tensor([0.7492])\n",
            " Train accuracy:  tensor([0.7497])\n",
            " Train accuracy:  tensor([0.7503])\n",
            " Train accuracy:  tensor([0.7508])\n",
            " Validation accuracy:  tensor([0.7059])\n",
            "Training loss after 4 epochs: 0.705774 Validation loss after 4 epochs: 0.874137 Time per epoch: 656.814843 seconds\n",
            " Train accuracy:  tensor([0.7523])\n",
            " Train accuracy:  tensor([0.7538])\n",
            " Train accuracy:  tensor([0.7536])\n",
            " Train accuracy:  tensor([0.7542])\n",
            " Train accuracy:  tensor([0.7550])\n",
            " Train accuracy:  tensor([0.7553])\n",
            " Train accuracy:  tensor([0.7558])\n",
            " Train accuracy:  tensor([0.7562])\n",
            " Train accuracy:  tensor([0.7566])\n",
            " Validation accuracy:  tensor([0.7020])\n",
            "Training loss after 5 epochs: 0.687890 Validation loss after 5 epochs: 0.899063 Time per epoch: 663.533692 seconds\n",
            " Train accuracy:  tensor([0.7588])\n",
            " Train accuracy:  tensor([0.7613])\n",
            " Train accuracy:  tensor([0.7608])\n",
            " Train accuracy:  tensor([0.7611])\n",
            " Train accuracy:  tensor([0.7617])\n",
            " Train accuracy:  tensor([0.7618])\n",
            " Train accuracy:  tensor([0.7621])\n",
            " Train accuracy:  tensor([0.7625])\n",
            " Train accuracy:  tensor([0.7629])\n",
            " Validation accuracy:  tensor([0.7018])\n",
            "Training loss after 6 epochs: 0.672051 Validation loss after 6 epochs: 0.920991 Time per epoch: 661.516969 seconds\n",
            " Train accuracy:  tensor([0.7634])\n",
            " Train accuracy:  tensor([0.7652])\n",
            " Train accuracy:  tensor([0.7648])\n",
            " Train accuracy:  tensor([0.7652])\n",
            " Train accuracy:  tensor([0.7658])\n",
            " Train accuracy:  tensor([0.7662])\n",
            " Train accuracy:  tensor([0.7666])\n",
            " Train accuracy:  tensor([0.7669])\n",
            " Train accuracy:  tensor([0.7671])\n",
            " Validation accuracy:  tensor([0.7010])\n",
            "Training loss after 7 epochs: 0.660087 Validation loss after 7 epochs: 0.937072 Time per epoch: 669.849391 seconds\n",
            " Train accuracy:  tensor([0.7679])\n",
            " Train accuracy:  tensor([0.7693])\n",
            " Train accuracy:  tensor([0.7688])\n",
            " Train accuracy:  tensor([0.7693])\n",
            " Train accuracy:  tensor([0.7700])\n",
            " Train accuracy:  tensor([0.7702])\n",
            " Train accuracy:  tensor([0.7705])\n",
            " Train accuracy:  tensor([0.7707])\n",
            " Train accuracy:  tensor([0.7709])\n",
            " Validation accuracy:  tensor([0.7018])\n",
            "Training loss after 8 epochs: 0.648465 Validation loss after 8 epochs: 0.946421 Time per epoch: 671.127044 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aadf419"
      },
      "source": [
        "## Testing performance\n",
        "\n",
        "We join the **train** and **validation** datasets and we train the model with this dataset using the optimal number of epochs. Then we evaluate the model with the **test** set. First we need to modify a bit the class used previously."
      ],
      "id": "2aadf419"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1bdb6d1"
      },
      "source": [
        "class CNN_extended(CNN):\n",
        "    \n",
        "    def __init__(self,nlabels, train_parameters, random_embeddings, epochs=100,lr=0.001):\n",
        "        \n",
        "        super().__init__(nlabels, train_parameters, random_embeddings)  \n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        \n",
        "        self.epochs = epochs\n",
        "        \n",
        "        self.criterion = nn.NLLLoss()               \n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "\n",
        "        self.valid_loss_during_training = []\n",
        "        \n",
        "    def trainloop(self,trainloader):\n",
        "        \n",
        "        \n",
        "        # Optimization Loop\n",
        "        \n",
        "        for e in range(int(self.epochs)):\n",
        "\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Random data permutation at each epoch\n",
        "            \n",
        "            running_loss = 0.\n",
        "            \n",
        "            i = 0\n",
        "            \n",
        "            length = 0\n",
        "            \n",
        "            accuracies = []\n",
        "            \n",
        "            for news, labels in trainloader:             \n",
        "        \n",
        "                self.optim.zero_grad()  # Reset gradients\n",
        "            \n",
        "                out = self.forward(news.int())\n",
        "\n",
        "                loss = self.criterion(out,labels.long())\n",
        "                \n",
        "                loss.backward()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                self.optim.step()\n",
        "                \n",
        "                top_p, top_class = out.topk(1, dim=1)\n",
        "                \n",
        "                equals = (top_class == labels.view(news.shape[0], 1))\n",
        "                \n",
        "                length += news.shape[0]\n",
        "                \n",
        "                accuracies.append(sum(equals)) \n",
        "                \n",
        "                accuracy = sum(accuracies)/length\n",
        "                \n",
        "                i += 1\n",
        "                \n",
        "                if i%1000 == 0:\n",
        "                    print(\" Train accuracy: \", accuracy)\n",
        "                \n",
        "            self.loss_during_training.append(running_loss/len(trainloader))\n",
        "            end_time = time.time()\n",
        "            print(\"Elapsed time: \", end_time-start_time)\n",
        "\n",
        "                \n",
        "    def eval_performance(self,dataloader):\n",
        "        predictions = np.empty((1,1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for news,labels in dataloader:\n",
        "                \n",
        "                logprobs = self.forward(news)  \n",
        "                top_p, top_class = logprobs.topk(1, dim=1)\n",
        "                \n",
        "                top_class_array = np.array(top_class)\n",
        "                predictions = np.concatenate((predictions, top_class_array), axis = 0)\n",
        "                \n",
        "        return predictions[1:]"
      ],
      "id": "c1bdb6d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "045b5f20"
      },
      "source": [
        "Now we join the train and validation sets."
      ],
      "id": "045b5f20"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d243cda"
      },
      "source": [
        "# Join train and validation sequences\n",
        "train_valid_tokenized_pad = np.concatenate((train_tokenized_pad, valid_tokenized_pad), axis = 0)\n",
        "# Join train and validation labels\n",
        "train_valid_labels = np.concatenate((np.array(train_labels), np.array(valid_labels)), axis = 0)\n",
        "\n",
        "# Create tensor objects\n",
        "\n",
        "# Train + validation\n",
        "train_valid_tensor = torch.Tensor(train_valid_tokenized_pad).int()\n",
        "# Test\n",
        "test_tensor =  torch.Tensor(test_tokenized_pad).int()\n",
        "\n",
        "# Tranform tensors into data loader objects\n",
        "\n",
        "# Train + validation\n",
        "train_valid_set = TensorDataset(train_valid_tensor, torch.Tensor(np.array(train_valid_labels)))\n",
        "train_valid_loader = DataLoader(train_valid_set, batch_size=60)\n",
        "# Test\n",
        "test_set = TensorDataset(test_tensor, torch.Tensor(np.array(test_labels)))\n",
        "testloader =  DataLoader(test_set, batch_size=60)"
      ],
      "id": "7d243cda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb3c5d4"
      },
      "source": [
        "### Random embeddings  + Training embeddings"
      ],
      "id": "4cb3c5d4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0e7b4f4",
        "outputId": "f4a2952f-d3da-4e7d-d4ae-fc7ccbd15f76"
      },
      "source": [
        "# Initialize model\n",
        "CNN_test_train_random = CNN_extended(nlabels = 6, epochs=1, lr=0.003, train_parameters = True, random_embeddings = True)\n",
        "# Train model\n",
        "CNN_test_train_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions1 = CNN_test_train_random.eval_performance(testloader)"
      ],
      "id": "f0e7b4f4",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5919])\n",
            " Train accuracy:  tensor([0.6198])\n",
            " Train accuracy:  tensor([0.6408])\n",
            " Train accuracy:  tensor([0.6534])\n",
            " Train accuracy:  tensor([0.6628])\n",
            " Train accuracy:  tensor([0.6697])\n",
            " Train accuracy:  tensor([0.6752])\n",
            " Train accuracy:  tensor([0.6800])\n",
            " Train accuracy:  tensor([0.6841])\n",
            " Train accuracy:  tensor([0.6874])\n",
            "Elapsed time:  7168.229678869247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f64b0527",
        "outputId": "3a5199fc-6a2d-4581-fd01-21a776e84aa1"
      },
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions1))"
      ],
      "id": "f64b0527",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.87      0.79     23507\n",
            "           1       0.63      0.26      0.37      3514\n",
            "           2       0.70      0.48      0.57     11297\n",
            "           3       0.72      0.07      0.13      1224\n",
            "           4       0.75      0.84      0.79     17472\n",
            "           5       0.71      0.54      0.61      2305\n",
            "\n",
            "    accuracy                           0.72     59319\n",
            "   macro avg       0.70      0.51      0.54     59319\n",
            "weighted avg       0.72      0.72      0.70     59319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZGve0Wp5XMu",
        "outputId": "25926e5f-2712-4589-ae1e-69d5abc01400"
      },
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions1, labels = [1,2,3,4,5]))"
      ],
      "id": "UZGve0Wp5XMu",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.26      0.37      3514\n",
            "           2       0.70      0.48      0.57     11297\n",
            "           3       0.72      0.07      0.13      1224\n",
            "           4       0.75      0.84      0.79     17472\n",
            "           5       0.71      0.54      0.61      2305\n",
            "\n",
            "   micro avg       0.73      0.62      0.67     35812\n",
            "   macro avg       0.70      0.44      0.49     35812\n",
            "weighted avg       0.72      0.62      0.65     35812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8c8aXFn5Wl1",
        "outputId": "bffd1e1f-6e75-4a7b-e7b6-1b41d909c33c"
      },
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions1))"
      ],
      "id": "g8c8aXFn5Wl1",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[20543   198  1057    12  1512   185]\n",
            " [ 1439   923   206     5   885    56]\n",
            " [ 3499   109  5391     7  2113   178]\n",
            " [  796    55    87    85   170    31]\n",
            " [ 1803   139   791     5 14672    62]\n",
            " [  706    51   131     4   172  1241]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe5ad30b"
      },
      "source": [
        "### GloVe embeddings  + Training embeddings"
      ],
      "id": "fe5ad30b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8797421",
        "outputId": "55f6012f-40fb-4ec6-c58f-ba47f1afa587"
      },
      "source": [
        "# Initialize model\n",
        "CNN_test_train_not_random = CNN_extended(nlabels = 6, epochs=1, lr=0.003, train_parameters = True, random_embeddings = False)\n",
        "# Train model\n",
        "CNN_test_train_not_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_2 = CNN_test_train_not_random.eval_performance(testloader)"
      ],
      "id": "c8797421",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.6673])\n",
            " Train accuracy:  tensor([0.6831])\n",
            " Train accuracy:  tensor([0.6921])\n",
            " Train accuracy:  tensor([0.6976])\n",
            " Train accuracy:  tensor([0.7028])\n",
            " Train accuracy:  tensor([0.7066])\n",
            " Train accuracy:  tensor([0.7096])\n",
            " Train accuracy:  tensor([0.7120])\n",
            " Train accuracy:  tensor([0.7144])\n",
            " Train accuracy:  tensor([0.7164])\n",
            "Elapsed time:  9102.66008424759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77e3853a",
        "outputId": "0df46398-5599-4153-f3fe-21866161a6bf"
      },
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_2))"
      ],
      "id": "77e3853a",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.87      0.80     23507\n",
            "           1       0.67      0.35      0.46      3514\n",
            "           2       0.71      0.54      0.61     11297\n",
            "           3       0.70      0.11      0.19      1224\n",
            "           4       0.76      0.83      0.80     17472\n",
            "           5       0.74      0.58      0.65      2305\n",
            "\n",
            "    accuracy                           0.74     59319\n",
            "   macro avg       0.72      0.55      0.58     59319\n",
            "weighted avg       0.73      0.74      0.72     59319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6xDZyYx5xEz",
        "outputId": "ba99f564-66b8-42cc-8c01-ffcbeffaf89a"
      },
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_2, labels = [1,2,3,4,5]))"
      ],
      "id": "x6xDZyYx5xEz",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.67      0.35      0.46      3514\n",
            "           2       0.71      0.54      0.61     11297\n",
            "           3       0.70      0.11      0.19      1224\n",
            "           4       0.76      0.83      0.80     17472\n",
            "           5       0.74      0.58      0.65      2305\n",
            "\n",
            "   micro avg       0.74      0.65      0.69     35812\n",
            "   macro avg       0.71      0.48      0.54     35812\n",
            "weighted avg       0.73      0.65      0.67     35812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umI_sor55xkl",
        "outputId": "c247199a-0237-4ff7-8dc9-a07f1bdd3f03"
      },
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_2))"
      ],
      "id": "umI_sor55xkl",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[20466   212  1159    33  1474   163]\n",
            " [ 1278  1223   147     6   806    54]\n",
            " [ 3044   131  6078    13  1862   169]\n",
            " [  738    44    94   131   188    29]\n",
            " [ 1679   181  1010     4 14541    57]\n",
            " [  633    45    93     1   192  1341]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ae61424"
      },
      "source": [
        "### Random embeddings  +  Not training embeddings"
      ],
      "id": "6ae61424"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e25f549b",
        "outputId": "7fa6bb9d-5b84-41a8-9f48-5806a7981cd1"
      },
      "source": [
        "# Initialize model\n",
        "CNN_test_not_train_random = CNN_extended(nlabels = 6, epochs=9, lr=0.003, train_parameters = False, random_embeddings = True)\n",
        "# Train model\n",
        "CNN_test_not_train_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_3 = CNN_test_not_train_random.eval_performance(testloader)"
      ],
      "id": "e25f549b",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.5530])\n",
            " Train accuracy:  tensor([0.5597])\n",
            " Train accuracy:  tensor([0.5638])\n",
            " Train accuracy:  tensor([0.5688])\n",
            " Train accuracy:  tensor([0.5730])\n",
            " Train accuracy:  tensor([0.5763])\n",
            " Train accuracy:  tensor([0.5796])\n",
            " Train accuracy:  tensor([0.5827])\n",
            " Train accuracy:  tensor([0.5856])\n",
            " Train accuracy:  tensor([0.5880])\n",
            "Elapsed time:  743.8465197086334\n",
            " Train accuracy:  tensor([0.6123])\n",
            " Train accuracy:  tensor([0.6118])\n",
            " Train accuracy:  tensor([0.6123])\n",
            " Train accuracy:  tensor([0.6127])\n",
            " Train accuracy:  tensor([0.6134])\n",
            " Train accuracy:  tensor([0.6141])\n",
            " Train accuracy:  tensor([0.6147])\n",
            " Train accuracy:  tensor([0.6150])\n",
            " Train accuracy:  tensor([0.6156])\n",
            " Train accuracy:  tensor([0.6159])\n",
            "Elapsed time:  741.4050281047821\n",
            " Train accuracy:  tensor([0.6202])\n",
            " Train accuracy:  tensor([0.6200])\n",
            " Train accuracy:  tensor([0.6203])\n",
            " Train accuracy:  tensor([0.6208])\n",
            " Train accuracy:  tensor([0.6211])\n",
            " Train accuracy:  tensor([0.6217])\n",
            " Train accuracy:  tensor([0.6220])\n",
            " Train accuracy:  tensor([0.6223])\n",
            " Train accuracy:  tensor([0.6228])\n",
            " Train accuracy:  tensor([0.6231])\n",
            "Elapsed time:  735.0587935447693\n",
            " Train accuracy:  tensor([0.6248])\n",
            " Train accuracy:  tensor([0.6242])\n",
            " Train accuracy:  tensor([0.6247])\n",
            " Train accuracy:  tensor([0.6246])\n",
            " Train accuracy:  tensor([0.6249])\n",
            " Train accuracy:  tensor([0.6252])\n",
            " Train accuracy:  tensor([0.6253])\n",
            " Train accuracy:  tensor([0.6254])\n",
            " Train accuracy:  tensor([0.6259])\n",
            " Train accuracy:  tensor([0.6261])\n",
            "Elapsed time:  742.421392917633\n",
            " Train accuracy:  tensor([0.6267])\n",
            " Train accuracy:  tensor([0.6266])\n",
            " Train accuracy:  tensor([0.6264])\n",
            " Train accuracy:  tensor([0.6265])\n",
            " Train accuracy:  tensor([0.6271])\n",
            " Train accuracy:  tensor([0.6275])\n",
            " Train accuracy:  tensor([0.6277])\n",
            " Train accuracy:  tensor([0.6279])\n",
            " Train accuracy:  tensor([0.6283])\n",
            " Train accuracy:  tensor([0.6284])\n",
            "Elapsed time:  757.4541792869568\n",
            " Train accuracy:  tensor([0.6299])\n",
            " Train accuracy:  tensor([0.6288])\n",
            " Train accuracy:  tensor([0.6288])\n",
            " Train accuracy:  tensor([0.6286])\n",
            " Train accuracy:  tensor([0.6289])\n",
            " Train accuracy:  tensor([0.6291])\n",
            " Train accuracy:  tensor([0.6292])\n",
            " Train accuracy:  tensor([0.6292])\n",
            " Train accuracy:  tensor([0.6295])\n",
            " Train accuracy:  tensor([0.6296])\n",
            "Elapsed time:  782.0601441860199\n",
            " Train accuracy:  tensor([0.6310])\n",
            " Train accuracy:  tensor([0.6302])\n",
            " Train accuracy:  tensor([0.6298])\n",
            " Train accuracy:  tensor([0.6298])\n",
            " Train accuracy:  tensor([0.6299])\n",
            " Train accuracy:  tensor([0.6303])\n",
            " Train accuracy:  tensor([0.6303])\n",
            " Train accuracy:  tensor([0.6302])\n",
            " Train accuracy:  tensor([0.6306])\n",
            " Train accuracy:  tensor([0.6306])\n",
            "Elapsed time:  836.1870765686035\n",
            " Train accuracy:  tensor([0.6324])\n",
            " Train accuracy:  tensor([0.6313])\n",
            " Train accuracy:  tensor([0.6309])\n",
            " Train accuracy:  tensor([0.6307])\n",
            " Train accuracy:  tensor([0.6308])\n",
            " Train accuracy:  tensor([0.6311])\n",
            " Train accuracy:  tensor([0.6311])\n",
            " Train accuracy:  tensor([0.6309])\n",
            " Train accuracy:  tensor([0.6312])\n",
            " Train accuracy:  tensor([0.6311])\n",
            "Elapsed time:  902.3521666526794\n",
            " Train accuracy:  tensor([0.6333])\n",
            " Train accuracy:  tensor([0.6323])\n",
            " Train accuracy:  tensor([0.6316])\n",
            " Train accuracy:  tensor([0.6315])\n",
            " Train accuracy:  tensor([0.6314])\n",
            " Train accuracy:  tensor([0.6317])\n",
            " Train accuracy:  tensor([0.6316])\n",
            " Train accuracy:  tensor([0.6315])\n",
            " Train accuracy:  tensor([0.6318])\n",
            " Train accuracy:  tensor([0.6318])\n",
            "Elapsed time:  902.0216271877289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f068b98",
        "outputId": "cbeb5654-2284-4081-ee94-eb126935d175"
      },
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_3))"
      ],
      "id": "8f068b98",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.81      0.71     23507\n",
            "           1       0.81      0.03      0.06      3514\n",
            "           2       0.52      0.34      0.41     11297\n",
            "           3       0.30      0.02      0.04      1224\n",
            "           4       0.67      0.78      0.72     17472\n",
            "           5       0.69      0.29      0.41      2305\n",
            "\n",
            "    accuracy                           0.63     59319\n",
            "   macro avg       0.60      0.38      0.39     59319\n",
            "weighted avg       0.63      0.63      0.59     59319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H34ptx3whpOS",
        "outputId": "4b0541c3-4649-4036-9da9-8038119847cc"
      },
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_3, labels = [1,2,3,4,5]))"
      ],
      "id": "H34ptx3whpOS",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.81      0.03      0.06      3514\n",
            "           2       0.52      0.34      0.41     11297\n",
            "           3       0.30      0.02      0.04      1224\n",
            "           4       0.67      0.78      0.72     17472\n",
            "           5       0.69      0.29      0.41      2305\n",
            "\n",
            "   micro avg       0.63      0.51      0.56     35812\n",
            "   macro avg       0.60      0.29      0.33     35812\n",
            "weighted avg       0.62      0.51      0.51     35812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1koxLr3NhpDw",
        "outputId": "baa0ed16-8c0c-42d3-fdec-787cf74cc78e"
      },
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_3))"
      ],
      "id": "1koxLr3NhpDw",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[19038    13  1838    22  2467   129]\n",
            " [ 2119   112   288     8   953    34]\n",
            " [ 4362     5  3845     3  2992    90]\n",
            " [  904     0   123    29   150    18]\n",
            " [ 2792     7   974    24 13640    35]\n",
            " [ 1093     2   307    11   221   671]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43a272b"
      },
      "source": [
        "### GloVe embeddings  +  Not training embeddings"
      ],
      "id": "e43a272b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "430ceef0",
        "outputId": "7e50a205-d631-4ca1-9c49-43798e839f2d"
      },
      "source": [
        "# Initialize model\n",
        "CNN_test_not_train_not_random = CNN_extended(nlabels = 6, epochs=2, lr=0.003, train_parameters = False, random_embeddings = False)\n",
        "# Train model\n",
        "CNN_test_not_train_not_random.trainloop(train_valid_loader)\n",
        "# Get predictions\n",
        "predictions_4 = CNN_test_not_train_not_random.eval_performance(testloader)"
      ],
      "id": "430ceef0",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train accuracy:  tensor([0.7470])\n",
            " Train accuracy:  tensor([0.7592])\n",
            " Train accuracy:  tensor([0.7651])\n",
            " Train accuracy:  tensor([0.7689])\n",
            " Train accuracy:  tensor([0.7726])\n",
            " Train accuracy:  tensor([0.7754])\n",
            " Train accuracy:  tensor([0.7776])\n",
            " Train accuracy:  tensor([0.7800])\n",
            " Train accuracy:  tensor([0.7822])\n",
            " Train accuracy:  tensor([0.7846])\n",
            "Elapsed time:  740.8222987651825\n",
            " Train accuracy:  tensor([0.7633])\n",
            " Train accuracy:  tensor([0.7730])\n",
            " Train accuracy:  tensor([0.7778])\n",
            " Train accuracy:  tensor([0.7817])\n",
            " Train accuracy:  tensor([0.7854])\n",
            " Train accuracy:  tensor([0.7881])\n",
            " Train accuracy:  tensor([0.7905])\n",
            " Train accuracy:  tensor([0.7925])\n",
            " Train accuracy:  tensor([0.7948])\n",
            " Train accuracy:  tensor([0.7971])\n",
            "Elapsed time:  733.8853116035461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b898225",
        "outputId": "ff5612e5-fb76-47ce-867b-f4f4da700c23"
      },
      "source": [
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_4))"
      ],
      "id": "4b898225",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.81      0.79     23507\n",
            "           1       0.60      0.40      0.48      3514\n",
            "           2       0.65      0.59      0.62     11297\n",
            "           3       0.35      0.21      0.26      1224\n",
            "           4       0.75      0.82      0.79     17472\n",
            "           5       0.71      0.59      0.64      2305\n",
            "\n",
            "    accuracy                           0.73     59319\n",
            "   macro avg       0.64      0.57      0.60     59319\n",
            "weighted avg       0.72      0.73      0.72     59319\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c50Y1ObIh35B",
        "outputId": "67442fcc-f816-4d2a-bb75-e994b337f279"
      },
      "source": [
        "# Classification report without 'true' label\n",
        "print(classification_report(np.array(test_labels).reshape(len(test_labels),1), predictions_4, labels = [1,2,3,4,5]))"
      ],
      "id": "c50Y1ObIh35B",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.60      0.40      0.48      3514\n",
            "           2       0.65      0.59      0.62     11297\n",
            "           3       0.35      0.21      0.26      1224\n",
            "           4       0.75      0.82      0.79     17472\n",
            "           5       0.71      0.59      0.64      2305\n",
            "\n",
            "   micro avg       0.70      0.67      0.69     35812\n",
            "   macro avg       0.61      0.52      0.56     35812\n",
            "weighted avg       0.69      0.67      0.68     35812\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtagSNrTh3yi",
        "outputId": "37c6e3a1-d41f-4279-f92d-15f294c3f9b2"
      },
      "source": [
        "# Confusion matrix\n",
        "print(confusion_matrix(np.array(test_labels).reshape(len(test_labels),1), predictions_4))"
      ],
      "id": "mtagSNrTh3yi",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[19078   369  1802   269  1765   224]\n",
            " [  998  1414   249    49   736    68]\n",
            " [ 2396   181  6675    72  1812   161]\n",
            " [  582    62   128   253   172    27]\n",
            " [ 1451   298  1291    44 14318    70]\n",
            " [  550    49   122    43   188  1353]]\n"
          ]
        }
      ]
    }
  ]
}